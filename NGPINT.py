#! /usr/bin/env python
#######################################################################################################
# Developed by: Sagnik Banerjee

# Version 1.7
# Modified to be used in conda environment
#
# Version: 1.6
# 
# This program will extract potential interactors of a certain Bait protein. Please
# launch python find_y2h_seq_candididates.py --help for more details of each possible functionality
# of the software. Please load the following modules before starting to run the program. Please note that 
# it is mandatory to preserve the sequence of the modules.
# 
# Updates from version 1.5
# Inclusion of logging system which logs information even with multiprocessing
#
# Updates from version 1.1:
# Now can process paired ended reads and leverage paired information during junction read detection
# 
# Updates from version 1.0:
# Only one csv file input is allowed
# STAR mappings to transcriptome is eliminated  
#    - Improves run time immensely
#    - No need to provide transcriptome index as input
#    - No need to provide gene annotations file as input (This will be generated by the pipeline)
#    - Users need to provide a GTF file though
# Option to generate transcript coverage graph is eliminated
# Fusion reads can now be viewed on IGV with vector sequence soft-clipped. From now on reads containing vector sequence will be referred to as Fusion reads consistently. 
# Produces much cleaner output and redirects all temporary files to another directory
# Fusion reads on IGV can now be viewed by grouping reads according to the tag "FR"
# Primer design is made easier with information available
# 
# BUG FIXES
# Version 1.5
# Trimming of reads have been modified - Each soft clipped read is checked for the presence of vector sequence for better fusion read recognition
# 
# Version 1.1
# Fixed a bug with reverse fusion read representation. The numbers in the header were incorrect. Now they have been corrected to represent the portion of the sequence that is NON-VECTOR
# Extracting the correct nucleotides after vector trimming. Previous version had one nucleotide off
# Fixed in-frame detection for cases where the mapping location was after the CDS end
# 
# TO DO
# Redesign the primer file. Output junction sequence for each transcript and each fragment. [Put in-frame junction reads in header]
#
# FEATURE ADDITION
# Version 1.4
#
# Add logging information through python logging module
# Version 1.5
# Better fusion read recognition
# No Logging
#
# Future Releases
#
# Incorporate Primer design softwares
# Restart computation from any point
# Impose memory restrictions
# Avoid recalculation in case of same background libraries
# Use pair information to prevent spurious hits for fusion reads
# Use modified model for enrichment analysis - both normalization and differential analysis
# Output HTML file for primer design
# Output complete sequences which were enriched 
#######################################################################################################



#######################################################################################################

"""# Defining the location of the executables
if sys.argv[0]!="find_y2h_seq_candidates_v1dot6.py":
    if platform.system()=="Linux":
        STAR="/".join(sys.argv[0].split("/")[:-1])+"/lib/star/Linux_x86_64/STAR "
    else:
        STAR="/".join(sys.argv[0].split("/")[:-1])+"/lib/star/MacOSX_x86_64/STAR "
    TRIMMOMATIC="/".join(sys.argv[0].split("/")[:-1])+"/lib/trimmomatic/trimmomatic-0.38.jar "
    SAMTOOLS="/".join(sys.argv[0].split("/")[:-1])+"/lib/samtools/samtools "
else:
    pass"""

from subprocess import *
import argparse
import collections
import datetime
import glob
import itertools
import math
import multiprocessing
import os
import pickle
import platform
import pprint
import re
import subprocess
import sys
import time

from ruffus.proxy_logger import *


def parseCommandLineArguments():
    """
    Parses the arguments provided through command line.
    Launch python find_y2h_seq_candidates.py --help for more details
    """
    parser = argparse.ArgumentParser(prog="NGPINT.py",description="This pipeline can be used to find potential interactors of the bait. Please make sure that the following softwares are available in your path. We recommend that you trim adapters from your libraries and then supply them as selected and/or background files.")
    optional_arg = parser.add_argument_group("Optional Arguments")
    required_arg = parser.add_argument_group("Required Arguments")
    
    parser.add_argument("--all_arguments","-a",help="Enter the csv file which has all the information. No other argument is needed. Do NOT change the order in which the entries appear in this file. Do not add or remove any entry. For optional arguments the last column can be left blank.",default=None,required=True)
    parser.add_argument('--version', action='version', version='%(prog)s 1.0')
    # SUPRESSED ARGUMENTS
    parser.add_argument("--selected_filename",help=argparse.SUPPRESS) # Separates the name of the selected file from its path
    parser.add_argument("--selected_path",help=argparse.SUPPRESS) # Extracts the path of each selected file
    parser.add_argument("--selected_sample_adapter_trimmed",help=argparse.SUPPRESS) # Adapter trimmed Filename
    parser.add_argument("--selected_sample_adapter_trimmed_error_file",help=argparse.SUPPRESS) # Error file for adapter trimming
    parser.add_argument("--selected_sample_N_removed",help=argparse.SUPPRESS) # Name of each selected file with all non-ATGC nucleotides removed
    parser.add_argument("--selected_sample_STAR_prefix_round1",help=argparse.SUPPRESS) # In Round1 all reads will be mapped to detect fusion reads. This is just the prefix NOT the filename
    parser.add_argument("--selected_sample_STAR_round1_output",help=argparse.SUPPRESS)
    parser.add_argument("--selected_sample_STAR_round1_error",help=argparse.SUPPRESS)
    parser.add_argument("--selected_sample_STAR_genome_filename_round1",help=argparse.SUPPRESS) # Bamfilename of the genome mapped file
    parser.add_argument("--selected_sample_STAR_transcriptome_bamfilename_round1",help=argparse.SUPPRESS) # Bamfilename of the transcriptome mapped file
    parser.add_argument("--selected_sample_STAR_prefix_round2",help=argparse.SUPPRESS) # In Round2 fusion reads which were mapped to the vectors will be trimmed and mapped back
    parser.add_argument("--selected_sample_STAR_round2_output",help=argparse.SUPPRESS)
    parser.add_argument("--selected_sample_STAR_round2_error",help=argparse.SUPPRESS)
    parser.add_argument("--selected_sample_STAR_genome_filename_round2",help=argparse.SUPPRESS) # Bamfilename of the genome mapped file
    parser.add_argument("--selected_sample_STAR_transcriptome_bamfilename_round2",help=argparse.SUPPRESS) # Bamfilename of the transcriptome mapped file
    parser.add_argument("--selected_sample_STAR_transcriptome_bamfilename_round2_fusion_reads",help=argparse.SUPPRESS) # Bamfilename of the transcriptome mapped file containing only fusion reads
    parser.add_argument("--selected_sample_trimming_stats",help=argparse.SUPPRESS) # Contains information related to vector trimming like number of reads trimmed etc.
    parser.add_argument("--selected_sample_all_reads_vector_trimmed",help=argparse.SUPPRESS) # Contains all the reads including vector trimmed ones and reads which do not contain any vector sequence
    parser.add_argument("--selected_sample_fusion_reads",help=argparse.SUPPRESS) # Contains only the fusion reads
    parser.add_argument("--selected_sample_genome_browser",help=argparse.SUPPRESS) # Samfilename for all reads to be viewed in Genome Browser
    parser.add_argument("--selected_sample_genome_browser_per_replicate",help=argparse.SUPPRESS) # Samfilename for all reads to be viewed in Genome Browser per replicate
    parser.add_argument("--selected_sample_salmon_counts_outputfile",help=argparse.SUPPRESS) # Counts file for salmon counts
    parser.add_argument("--selected_sample_salmon_counts_error",help=argparse.SUPPRESS) # Error file for salmon counts
    parser.add_argument("--selected_sample_transcriptome_coverage_bed_all_reads",help=argparse.SUPPRESS) # Bed file to store the mappings counts for each transcript
    parser.add_argument("--selected_sample_transcriptome_coverage_bed_fusion_reads",help=argparse.SUPPRESS)
    parser.add_argument("--selected_sample_transcriptome_coverage_bed_all_reads_splits",help=argparse.SUPPRESS) # Bed file to store the mappings counts for each transcript
    parser.add_argument("--selected_sample_transcriptome_coverage_bed_fusion_reads_splits",help=argparse.SUPPRESS) 
    parser.add_argument("--selected_sample_idxstats_filename_all_reads",help=argparse.SUPPRESS) # Idxstats file for storing number of reads mapped to a transcript
    parser.add_argument("--selected_sample_idxstats_filename_fusion_reads",help=argparse.SUPPRESS) # Idxstats file for storing number of fusion reads mapped to a transcript
    parser.add_argument("--selected_sample_graph_info_filename",help=argparse.SUPPRESS) # Information about graphs
    parser.add_argument("--selected_sample_for_cross_library_analysis",help=argparse.SUPPRESS) # selected_sample_for_cross_library_analysis
    parser.add_argument("--selected_sample_amplicon_filename",help=argparse.SUPPRESS)
    parser.add_argument("--selected_sample_transcript_read_coverage",help=argparse.SUPPRESS)
    parser.add_argument("--selected_sample_per_read_log",help=argparse.SUPPRESS)# Log to describe each read
    
    parser.add_argument("--background_sample",help=argparse.SUPPRESS) # Separates the name of the selected file from its path
    parser.add_argument("--background_path",help=argparse.SUPPRESS) # Extracts the path of each selected file
    parser.add_argument("--background_sample_adapter_trimmed",help=argparse.SUPPRESS) # Adapter trimmed Filename
    parser.add_argument("--background_sample_adapter_trimmed_error_file",help=argparse.SUPPRESS) # Error file for adapter trimming
    parser.add_argument("--background_sample_N_removed",help=argparse.SUPPRESS) # Name of each selected file with all non-ATGC nucleotides removed
    parser.add_argument("--background_sample_STAR_prefix_round1",help=argparse.SUPPRESS) # In Round1 all reads will be mapped to detect fusion reads. This is just the prefix NOT the filename
    parser.add_argument("--background_sample_STAR_round1_output",help=argparse.SUPPRESS)
    parser.add_argument("--background_sample_STAR_round1_error",help=argparse.SUPPRESS)
    parser.add_argument("--background_sample_STAR_genome_bamfilename_round1",help=argparse.SUPPRESS) # Bamfilename of the genome mapped file
    parser.add_argument("--background_sample_STAR_genome_filename_round1",help=argparse.SUPPRESS) # Bamfilename of the transcriptome mapped file
    parser.add_argument("--background_sample_STAR_prefix_round2",help=argparse.SUPPRESS) # In Round2 fusion reads which were mapped to the vectors will be trimmed and mapped back
    parser.add_argument("--background_sample_STAR_round2_output",help=argparse.SUPPRESS)
    parser.add_argument("--background_sample_STAR_round2_error",help=argparse.SUPPRESS)
    parser.add_argument("--background_sample_STAR_genome_filename_round2",help=argparse.SUPPRESS) # Bamfilename of the genome mapped file
    parser.add_argument("--background_sample_STAR_transcriptome_bamfilename_round2",help=argparse.SUPPRESS) # Bamfilename of the transcriptome mapped file
    parser.add_argument("--background_sample_STAR_transcriptome_bamfilename_round2_fusion_reads",help=argparse.SUPPRESS) # Bamfilename of the transcriptome mapped file containing only fusion reads
    parser.add_argument("--background_sample_trimming_stats",help=argparse.SUPPRESS) # Contains information related to vector trimming like number of reads trimmed etc.
    parser.add_argument("--background_sample_all_reads_vector_trimmed",help=argparse.SUPPRESS) # Contains all the reads including vector trimmed ones and reads which do not contain any vector sequence
    parser.add_argument("--background_sample_fusion_reads",help=argparse.SUPPRESS) # Contains only the fusion reads
    parser.add_argument("--background_sample_salmon_counts_outputfile",help=argparse.SUPPRESS) # Counts file for salmon counts
    parser.add_argument("--background_sample_salmon_counts_error",help=argparse.SUPPRESS) # Error file for salmon counts
    parser.add_argument("--background_sample_per_read_log",help=argparse.SUPPRESS)# Log to describe each read
    
    parser.add_argument("--temp_output_directory",help=argparse.SUPPRESS)
    parser.add_argument("--transcript_to_gene_map",help=argparse.SUPPRESS)
    parser.add_argument("--transcriptome",help=argparse.SUPPRESS) # Name of the transcriptome file generated by gffread
    parser.add_argument("--transcriptome_index",help=argparse.SUPPRESS) # STAR index of the transcriptome
    parser.add_argument("--salmon_normalized_counts",help=argparse.SUPPRESS)
    parser.add_argument("--salmon_DGE_filename",help=argparse.SUPPRESS)
    parser.add_argument("--salmon_gene_counts_matrix",help=argparse.SUPPRESS) # Combined file containing all the count data
    parser.add_argument("--deseq2_normalized_counts",help=argparse.SUPPRESS) # File containing normalized values
    parser.add_argument("--deseq2_DGE_output",help=argparse.SUPPRESS) # Output file from DESeq2
    parser.add_argument("--os",help=argparse.SUPPRESS)
    parser.add_argument("--combined_graph_final",help=argparse.SUPPRESS) # Contains all the information about each enrichment of genes in each of the replicates
    parser.add_argument("--run_details_info_csv",help=argparse.SUPPRESS) # Holds information about the runtime of each of the steps and other relevant information 
    parser.add_argument("--record_time",help=argparse.SUPPRESS)# Holds duration of execution for each step of execution
    parser.add_argument("--design_primers_for_transcript",help=argparse.SUPPRESS)# The filename where all transcripts are displayed with in-frame junction information required for primer design
    
    return parser.parse_args()

def populateOptionsDataStructure(options,filename):
    """
    Populates the options with data provided by the use
    """
    whole_data=[line.strip().split(",") for line in open(filename,"r",encoding="ISO-8859-1").read().split("\n")[1:]]
    """for line in whole_data:
        print(line)"""
    #print(whole_data[4])
    options.selected_ended=whole_data[0][3] if whole_data[0][4]=="" else whole_data[0][4] 
    #options.selected_fullpath=[file.strip("\"") for file in whole_data[1][4:].split(";")]
    options.selected_fullpath=whole_data[1][4].split(";")
    options.background_ended=whole_data[2][3] if whole_data[2][4]=="" else whole_data[2][4]
    #options.background_fullpath=[file.strip("\"") for file in whole_data[3][4:].split(";")]
    options.background_fullpath=whole_data[3][4].split(";")
    options.genome=whole_data[4][4]
    options.star_genome_index=whole_data[5][4] if whole_data[5][4]!="" else None  
    options.output_directory=whole_data[6][4] 
    options.plasmid_sequences=whole_data[7][4]
    options.five_prime_vector=whole_data[8][4]
    options.three_prime_vector=whole_data[9][4]
    options.CPU=int(whole_data[10][4]) if whole_data[10][4]!="" else whole_data[10][3]  
    options.frame_of_TF_fusion=whole_data[11][4] if whole_data[11][4]!="" else whole_data[11][3] 
    options.nucleotide_for_frame_shift=whole_data[12][4] if whole_data[12][4]!="" else whole_data[12][3] 
    options.min_trimmed_length=whole_data[13][4] if whole_data[13][4]!="" else whole_data[13][3]
    options.min_trimmed_length=int(options.min_trimmed_length) 
    options.force=whole_data[14][4] if whole_data[14][4]!="" else whole_data[14][3] 
    options.clean_up=whole_data[15][4] if whole_data[15][4]!="" else whole_data[15][3]
    options.gtf=whole_data[16][4] 
    options.functional_annotation=whole_data[17][4]
    options.transcriptome_index=whole_data[18][4]
    return options

def analyzeCommandLineArguments(options,logger_proxy,logging_mutex):
    """
    Performs checks on the validity of the arguments provided 
    through the command line.
    """
    """for key in vars(options):
        if vars(options)[key] is not None:
            print(key,vars(options)[key])"""
    
    flag=0
    if platform.system()=="Linux":
        options.os="linux"
    elif platform.system()=="Darwin":
        options.os="mac"
    else:
        options.os="Invalid"
        os.system("echo \" You need Linux or Mac to execute the software\"")
        flag=1
        
    if options.selected_ended==options.background_ended:
        if len(options.selected_fullpath)!=len(options.background_fullpath):
            with logging_mutex:
                logger_proxy.info("You must have equal number of selected and non-selected libraries. EXECUTION STOPPED")
            #logger.error("You must have equal number of selected and non-selected libraries. EXECUTION STOPPED")
            flag=1
    else:
        if options.selected_ended=="SE" and options.background_ended=="PE" and len(options.selected_fullpath)*2!=len(options.background_fullpath):
            #logger.error("For every selected sample you must have 2 non-selected samples. EXECUTION STOPPED")
            with logging_mutex:
                logger_proxy.info("For every selected sample you must have 2 non-selected samples. EXECUTION STOPPED")
            flag=1
        if options.selected_ended=="PE" and options.background_ended=="SE" and len(options.selected_fullpath)!=2*len(options.background_fullpath):
            #logger.error("For every non-selected sample you must have 2 selected samples. EXECUTION STOPPED")
            with logging_mutex:
                logger_proxy.info("For every non-selected sample you must have 2 selected samples. EXECUTION STOPPED")
            flag=1
    
    if os.path.exists(options.genome)==False:
        #logger.error("The genome file you provided does not exist. EXECUTION STOPPED")
        with logging_mutex:
            logger_proxy.info("The genome file you provided does not exist. EXECUTION STOPPED "+options.genome)
        flag=1
        
    for filename in options.selected_fullpath:
        if os.path.exists(filename)==False:
            #logger.error("The selected file "+filename+" does not exist. EXECUTION STOPPED")
            with logging_mutex:
                logger_proxy.info("The selected file "+filename+" does not exist. EXECUTION STOPPED")
            flag=1
    
    for filename in options.background_fullpath:
        if os.path.exists(filename)==False:
            #logger.error("The background file "+filename+" does not exist. EXECUTION STOPPED")
            with logging_mutex:
                logger_proxy.info("The background file "+filename+" does not exist. EXECUTION STOPPED")
            flag=1
    
    options.temp_output_directory=options.output_directory+"/temp"
    os.system("mkdir -p "+options.temp_output_directory)
    fhw=open(options.temp_output_directory+"/vector_sequences.fasta","w")
    fhw.write(">5_prime_end\n"+options.five_prime_vector+"\n>3_prime_end\n"+options.three_prime_vector+"\n")
    fhw.close()
    options.vector_sequences=options.temp_output_directory+"/vector_sequences.fasta"
    
    if flag==1:
        print("The program had to terminate prematurely....Please check "+options.output_directory+"/progress.log file for more details")
        sys.stdout.flush()
        sys.exit()
        
    if options.star_genome_index is None or options.star_genome_index=="":
        os.system("rm -rf "+options.temp_output_directory+"/star_index_with_vectors_with_transcriptome.temp")
        
        cmd="mkdir -p "+options.temp_output_directory+"/star_index_with_vectors_with_transcriptome"
        os.system(cmd)
        
        cmd="STAR "
        cmd+=" --runThreadN "+str(options.CPU)
        cmd+=" --runMode genomeGenerate "
        cmd+=" --genomeDir "+options.temp_output_directory+"/star_index_with_vectors_with_transcriptome"
        cmd+=" --genomeFastaFiles "+options.genome+" "+options.plasmid_sequences+" "+options.vector_sequences
        cmd+=" --sjdbGTFfile "+options.gtf
        cmd+=" --outTmpDir "+options.temp_output_directory+"/star_index_with_vectors_with_transcriptome.temp "
        cmd+=" > "+options.temp_output_directory+"/star_index_with_vectors_with_transcriptome.output"
        cmd+=" 2> "+options.temp_output_directory+"/star_index_with_vectors_with_transcriptome.error"
        os.system(cmd)
        
        options.star_genome_index=options.temp_output_directory+"/star_index_with_vectors_with_transcriptome"
        with logging_mutex:
            logger_proxy.info("STAR index generation complete")
        """cmd="cat "+options.temp_output_directory+"/star_index_with_vectors_with_transcriptome.error >> "+options.output_directory+"/progress.log"
        os.system(cmd)"""
    
    
    options.transcriptome=options.temp_output_directory+"/transcriptome.fasta"
    cmd="gffread "+options.gtf+" -g "+options.genome+" -w "+options.transcriptome
    os.system(cmd)
    
    cmd="perl -pe '/^>/ ? print \"\n\" : chomp' "+options.transcriptome+"|tail -n +2 > "+options.transcriptome+".temp"
    os.system(cmd)
    
    cmd="mv "+options.transcriptome+".temp "+options.transcriptome
    os.system(cmd)
    
    # Creating STAR index for the transcriptome
    if options.transcriptome_index is None or options.transcriptome_index=="":
        os.system("mkdir -p "+options.temp_output_directory+"/star_index_transcriptome")
        
        os.system("rm -rf "+options.temp_output_directory+"/star_index_transcriptome.temp ")
        cmd="STAR "
        cmd+=" --runThreadN "+str(options.CPU)
        cmd+=" --runMode genomeGenerate "
        cmd+=" --genomeDir "+options.temp_output_directory+"/star_index_transcriptome "
        cmd+=" --genomeFastaFiles "+options.transcriptome
        cmd+=" --limitGenomeGenerateRAM 37757950325 "
        cmd+=" --outTmpDir "+options.temp_output_directory+"/star_index_transcriptome.temp "
        cmd+=" > "+options.temp_output_directory+"/star_index_transcriptome.output "
        cmd+=" 2> "+options.temp_output_directory+"/star_index_transcriptome.error "
        os.system(cmd)
        options.transcriptome_index=options.temp_output_directory+"/star_index_transcriptome"
    
    options.transcript_to_gene_map=options.temp_output_directory+"/transcript_to_gene_map"
    # Defining the SUPPRESSED arguments
    options.selected_path=[]
    for selected_lib_path in options.selected_fullpath:
        options.selected_path.append("/".join(selected_lib_path.split("/")[:-1]))
    options.selected_sample=[]
    for filenames in options.selected_fullpath:
        options.selected_sample.append(filenames.split("/")[-1].split(".")[0])
        
    options.background_path=[]
    for background_lib_path in options.background_fullpath:
        options.background_path.append("/".join(background_lib_path.split("/")[:-1]))
    options.background_sample=[]
    for filename in options.background_fullpath:
        options.background_sample.append(filename.split("/")[-1].split(".")[0])
    
    if os.path.exists(options.genome+".fai")==False:
        cmd="samtools faidx "+options.genome
        os.system(cmd)
    
    options.selected_sample_adapter_trimmed=[]
    options.selected_sample_adapter_trimmed_error_file=[]
    options.selected_sample_N_removed=[]
    options.selected_sample_STAR_prefix_round1=[]
    options.selected_sample_STAR_round1_output=[]
    options.selected_sample_STAR_round1_error=[]
    options.selected_sample_STAR_genome_filename_round1=[]
    options.selected_sample_STAR_transcriptome_bamfilename_round1=[]
    options.selected_sample_STAR_prefix_round2=[]
    options.selected_sample_STAR_round2_output=[]
    options.selected_sample_STAR_round2_error=[]
    options.selected_sample_STAR_genome_filename_round2=[]
    options.selected_sample_STAR_transcriptome_bamfilename_round2=[]
    options.selected_sample_STAR_transcriptome_bamfilename_round2_fusion_reads=[]
    options.selected_sample_trimming_stats=[]
    options.selected_sample_all_reads_vector_trimmed=[]
    options.selected_sample_fusion_reads=[]
    options.selected_sample_salmon_counts_outputfile=[]
    options.selected_sample_salmon_counts_error=[]
    options.selected_sample_transcriptome_coverage_bed_all_reads=[]
    options.selected_sample_transcriptome_coverage_bed_fusion_reads=[]
    options.selected_sample_idxstats_filename_all_reads=[]
    options.selected_sample_idxstats_filename_fusion_reads=[]
    options.selected_sample_transcript_read_coverage=[]
    options.selected_sample_graph_info_filename=[]
    options.selected_sample_for_cross_library_analysis=[]
    options.selected_sample_amplicon_filename=[]
    options.selected_sample_genome_browser_per_replicate=[]
    options.selected_filename=options.selected_sample
    options.selected_sample_per_read_log=[]
    for filename in options.selected_sample:
        options.selected_sample_adapter_trimmed.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_adapter_trimmed.fastq")
        options.selected_sample_adapter_trimmed_error_file.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_adapter_trimmed.error")
        options.selected_sample_N_removed.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_N_removed.fastq")
        options.selected_sample_STAR_prefix_round1.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1_")
        options.selected_sample_STAR_round1_output.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1.output")
        options.selected_sample_STAR_round1_error.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1.error")
        options.selected_sample_STAR_genome_filename_round1.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1_Aligned.out.sam")
        options.selected_sample_STAR_transcriptome_bamfilename_round1.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1_Aligned.toTranscriptome.out.bam")
        options.selected_sample_STAR_prefix_round2.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_")
        options.selected_sample_STAR_round2_output.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2.output")
        options.selected_sample_STAR_round2_error.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2.error")
        options.selected_sample_STAR_genome_filename_round2.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_Aligned.out.sam")
        options.selected_sample_STAR_transcriptome_bamfilename_round2.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_Aligned.toTranscriptome.out.bam")
        options.selected_sample_STAR_transcriptome_bamfilename_round2_fusion_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_fusion_reads_Aligned.toTranscriptome.out.bam")
        options.selected_sample_trimming_stats.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_trim.stats")
        options.selected_sample_all_reads_vector_trimmed.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_trimmed_reads.fastq")
        options.selected_sample_fusion_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_fusion_reads.fastq")
        options.selected_sample_salmon_counts_error.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_salmon.error")
        options.selected_sample_salmon_counts_outputfile.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_salmon_counts")
        options.selected_sample_transcriptome_coverage_bed_all_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_forGraphs_trimmed_reads.bed")
        options.selected_sample_transcriptome_coverage_bed_fusion_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_forGraphs_fusion_reads.bed")
        options.selected_sample_idxstats_filename_all_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_transcriptome_Aligned.out.sorted.idxstats")
        options.selected_sample_idxstats_filename_fusion_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_transcriptome_fusion_reads_Aligned.out.sorted.idxstats")
        options.selected_sample_transcript_read_coverage.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_transcript_read_coverage.txt") # Filename to store the selected transcripts based on the number of reads that are mapped to that transcript
        options.selected_sample_graph_info_filename.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_graphs.tab")
        options.selected_sample_for_cross_library_analysis.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_for_cross_library_analysis.csv")
        options.selected_sample_amplicon_filename.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_full_length_amplicons_with_vectors.fasta")
        options.selected_sample_genome_browser_per_replicate.append(options.output_directory+"/"+filename.split(".fastq")[0]+"_genome_browser.bam")
        options.selected_sample_per_read_log.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_fusion_read_info.log")
        
    options.background_sample_adapter_trimmed=[]
    options.background_sample_adapter_trimmed_error_file=[]
    options.background_sample_N_removed=[]
    options.background_sample_STAR_prefix_round1=[]
    options.background_sample_STAR_round1_output=[]
    options.background_sample_STAR_round1_error=[]
    options.background_sample_STAR_genome_filename_round1=[]
    options.background_sample_STAR_transcriptome_bamfilename_round1=[]
    options.background_sample_STAR_prefix_round2=[]
    options.background_sample_STAR_round2_output=[]
    options.background_sample_STAR_round2_error=[]
    options.background_sample_STAR_genome_filename_round2=[]
    options.background_sample_STAR_transcriptome_bamfilename_round2=[]
    options.background_sample_trimming_stats=[]
    options.background_sample_all_reads_vector_trimmed=[]
    options.background_sample_fusion_reads=[]
    options.background_sample_salmon_counts_outputfile=[]
    options.background_sample_salmon_counts_error=[]
    options.background_sample_transcriptome_coverage_bed_all_reads=[]
    options.background_sample_transcriptome_coverage_bed_fusion_reads=[]
    options.background_sample_STAR_transcriptome_bamfilename_round2_fusion_reads=[]
    options.background_sample=options.background_sample
    options.background_sample_per_read_log=[]

    for filename in options.background_sample:
        options.background_sample_adapter_trimmed.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_adapter_trimmed.fastq")
        options.background_sample_adapter_trimmed_error_file.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_adapter_trimmed.error")
        options.background_sample_N_removed.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_N_removed.fastq")
        options.background_sample_STAR_prefix_round1.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1_")
        options.background_sample_STAR_round1_output.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1.output")
        options.background_sample_STAR_round1_error.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1.error")
        options.background_sample_STAR_genome_filename_round1.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1_Aligned.out.sam")
        options.background_sample_STAR_transcriptome_bamfilename_round1.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round1_Aligned.toTranscriptome.out.bam")
        options.background_sample_STAR_prefix_round2.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_")
        options.background_sample_STAR_round2_output.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2.output")
        options.background_sample_STAR_round2_error.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2.error")
        options.background_sample_STAR_genome_filename_round2.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_Aligned.out.sam")
        options.background_sample_STAR_transcriptome_bamfilename_round2.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_Aligned.toTranscriptome.out.bam")
        options.background_sample_trimming_stats.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_trim.stats")
        options.background_sample_all_reads_vector_trimmed.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_trimmed_reads.fastq")
        options.background_sample_fusion_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_fusion_reads.fastq")
        options.background_sample_salmon_counts_error.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_salmon.error")
        options.background_sample_salmon_counts_outputfile.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_salmon_counts")
        options.background_sample_transcriptome_coverage_bed_all_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_forGraphs_trimmed_reads.bed")
        options.background_sample_transcriptome_coverage_bed_fusion_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_forGraphs_fusion_reads.bed")
        options.background_sample_STAR_transcriptome_bamfilename_round2_fusion_reads.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_STAR_round2_fusion_reads_Aligned.toTranscriptome.out.bam")
        options.background_sample_per_read_log.append(options.temp_output_directory+"/"+filename.split(".fastq")[0]+"_fusion_read_info.log")
        
    options.selected_sample_genome_browser=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_genome_browser.bam"
    options.combined_graph_temp=options.output_directory+"/"+options.temp_output_directory.split("/")[-1]+"_combined_graphs_temp.tab"
    options.combined_graph_final=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_final_report.csv"
    options.combined_graph=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_combined_graphs.tab"
    options.salmon_normalized_counts=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_salmon_counts.matrix_norm.csv"
    options.salmon_DGE_filename=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_salmon_counts.matrix_DGE_Indept_filtering_Off_Cooks_Off.csv"
    
    options.salmon_gene_counts_matrix=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_salmon_counts.matrix"
    options.deseq2_DGE_output=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_salmon_counts.matrix_DGE_Indept_filtering_On_Cooks_On.csv"
    options.deseq2_normalized_counts=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_salmon_counts.matrix_norm.csv"
    options.run_details_info_csv=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_run_details.csv"
    options.frame_of_TF_fusion=int(options.frame_of_TF_fusion)
    options.record_time={}
    options.design_primers_for_transcript=options.output_directory+"/"+options.output_directory.split("/")[-1]+"_transcriptome_file_for_primer_design.txt"
    
    return options

def isKthBitSet(n, k): 
    if n & (1 << (k - 1)): 
        return True 
    else: 
        return False 
    
def writeFastqFile(filename,reads):
    fhw=open(filename,"w")
    for read in reads:
        fhw.write("@"+read+"\n")
        fhw.write(reads[read][0]+"\n"+reads[read][1]+"\n"+reads[read][2]+"\n")

def writeFastaFile(filename,seqs):
    fhw=open(filename,"w")
    for id in seqs:
        fhw.write(">"+id+"\n"+seqs[id]+"\n")

def readFastqFile(filename):
    reads={}
    fhr=open(filename,"r")
    while True:
        line=fhr.readline()
        if not line:
            break
        reads[line.split()[0][1:]]=[fhr.readline().strip(),fhr.readline().strip(),fhr.readline().strip()]
        #print(reads[line.split()[0]])
    return reads

def readFastaFile(filename):
    """
    Reads in a fasta file and returns a dictionary
    The keys in the dictionary is same as the fasta header
    for each sequence upto the first space.
    """
    info={}
    fhr=open(filename,"r")
    while(True):
        line=fhr.readline()
        if not line: break
        if(">" in line):
            try:
                info[line.strip()[1:].split()[0]]=fhr.readline().strip()
            except ValueError:
                pass
    return info

def convertToDateAndTime(seconds):
    """
    Returns a string with hours, minutes and seconds
    """
    hours=seconds//3600
    minutes=(seconds%3600)//60
    seconds=(seconds%60)
    return str(hours)+" hrs, "+str(minutes)+" min, "+str(seconds)+" seconds"

def allindices(string, sub, offset=0):
    listindex=[]
    i = string.find(sub, offset)
    while i >= 0:
        listindex.append(i)
        i = string.find(sub, i + 1)
    return listindex

def findTranscriptsWithSTOPCodonsIn5PrimeUTRSequence(options,gene_info):
    """
    Finds transcripts with STOP codons in 5' UTR sequences
    """
    transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region={}
    fhr=open(options.transcriptome,"r")
    for line in fhr:
        if ">" in line:
            transcript_id=line.strip().split()[0][1:]
            if transcript_id not in gene_info:continue
            #start,end=int(line.strip().split("CDS=")[-1].split("-")[0]),int(line.strip().split("CDS=")[-1].split("-")[-1])
            start,end=gene_info[transcript_id]["cds_start"],gene_info[transcript_id]["cds_end"]
            seq=fhr.readline().strip()
            five_prime_UTR_region=seq[:start-1]
            if len(five_prime_UTR_region) % 3 == 1:
                five_prime_UTR_region=five_prime_UTR_region[1:]
                dropped=1
            elif len(five_prime_UTR_region) % 3 == 2:
                five_prime_UTR_region=five_prime_UTR_region[2:]
                dropped=2
            else:
                dropped=0
            locations_of_TAA=allindices(five_prime_UTR_region,"TAA")
            locations_of_TAG=allindices(five_prime_UTR_region,"TAG")
            locations_of_TGA=allindices(five_prime_UTR_region,"TGA")
            locations_of_TAA_in_frame_with_CDS_check=[x%3 for x in locations_of_TAA]
            locations_of_TAG_in_frame_with_CDS_check=[x%3 for x in locations_of_TAG]
            locations_of_TGA_in_frame_with_CDS_check=[x%3 for x in locations_of_TGA]
            if dropped==1:
                locations_of_TAA_in_frame_with_CDS=[loc+1+1 for num,loc in enumerate(locations_of_TAA) if locations_of_TAA_in_frame_with_CDS_check[num]==0]
                locations_of_TAG_in_frame_with_CDS=[loc+1+1 for num,loc in enumerate(locations_of_TAG) if locations_of_TAG_in_frame_with_CDS_check[num]==0]
                locations_of_TGA_in_frame_with_CDS=[loc+1+1 for num,loc in enumerate(locations_of_TGA) if locations_of_TGA_in_frame_with_CDS_check[num]==0]
            elif dropped==2:
                locations_of_TAA_in_frame_with_CDS=[loc+2+1 for num,loc in enumerate(locations_of_TAA) if locations_of_TAA_in_frame_with_CDS_check[num]==0]
                locations_of_TAG_in_frame_with_CDS=[loc+2+1 for num,loc in enumerate(locations_of_TAG) if locations_of_TAG_in_frame_with_CDS_check[num]==0]
                locations_of_TGA_in_frame_with_CDS=[loc+2+1 for num,loc in enumerate(locations_of_TGA) if locations_of_TGA_in_frame_with_CDS_check[num]==0]
            else:
                locations_of_TAA_in_frame_with_CDS=[loc+1 for num,loc in enumerate(locations_of_TAA) if locations_of_TAA_in_frame_with_CDS_check[num]==0]
                locations_of_TAG_in_frame_with_CDS=[loc+1 for num,loc in enumerate(locations_of_TAG) if locations_of_TAG_in_frame_with_CDS_check[num]==0]
                locations_of_TGA_in_frame_with_CDS=[loc+1 for num,loc in enumerate(locations_of_TGA) if locations_of_TGA_in_frame_with_CDS_check[num]==0]
            all_locations=[]
            all_locations.extend(locations_of_TAA_in_frame_with_CDS)
            all_locations.extend(locations_of_TAG_in_frame_with_CDS)
            all_locations.extend(locations_of_TGA_in_frame_with_CDS)
            transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region[transcript_id]=sorted(all_locations)
    fhr.close()
    return transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region
 
def getGeneInfo(transcriptome_filename,transcript_to_gene_map_filename):
    """
    """
    data={}
    fhr=open(transcript_to_gene_map_filename,"r")
    for line in fhr:
        transcript,gene=line.split()
        data[transcript]={"gene":gene}
    fhr.close()
    
    fhr=open(transcriptome_filename,"r")
    for line in fhr:
        if ">" in line:
            transcript=line[1:].split()[0]
            if "CDS=" in line:
                cds_start,cds_end=line[1:].split("CDS=")[-1].split("-")[0],line[1:].split("CDS=")[-1].split("-")[1]
                data[transcript]["cds_start"]=int(cds_start)
                data[transcript]["cds_end"]=int(cds_end)
            """else:
                data[transcript]["cds_start"]=-1
                #data[transcript]["cds_end"]=len(fhr.readline().strip())
                data[transcript]["cds_end"]=-1"""
    fhr.close()
    
    all_transcripts=list(data.keys())
    for transcript in all_transcripts:
        if "cds_start" not in data[transcript]:
            del data[transcript]
    return data

def generateTranscriptToGeneMap(gtffilename,options):
    """
    """
    cmd="paste <(cat "+gtffilename+" |rev|cut -f1|rev|cut -d\" \" -f2|cut -c2-|rev|cut -c3-|rev) "
    cmd+="<(cat "+gtffilename+" |rev|cut -f1|rev|cut -d\" \" -f4|cut -c2-|rev|cut -c3-|rev)|sort|uniq "
    cmd+=" > "+options.transcript_to_gene_map
    subprocess.check_call(['bash', '-c', cmd])
 
def runTrimmomaticToTrimOffAdapters(options,logger_proxy,logging_mutex):
    """
    No quality trimming will be done. Only removal of 
    adapter sequence is essential
    """
    for num,eachtype in enumerate([options.selected_filename,options.background_sample]):
        for file_num,filename in enumerate(eachtype):
            if num==0:
                if options.selected_path[file_num]=="":
                    inputfilename=filename+".fastq "
                else:
                    inputfilename=options.selected_path[file_num]+"/"+filename+".fastq "
                outputfilename=options.selected_sample_adapter_trimmed[file_num]
                errorfilename=options.selected_sample_adapter_trimmed_error_file[file_num]
            else:
                if options.background_path[file_num]=="":
                    inputfilename=filename+".fastq "
                else:
                    inputfilename=options.background_path[file_num]+"/"+filename+".fastq "
                outputfilename=options.background_sample_adapter_trimmed[file_num]
                errorfilename=options.background_sample_adapter_trimmed_error_file[file_num]
            #print(filename,outputfilename)
            #cmd="java -jar "
            cmd="trimmomatic SE -phred33 -threads "+str(options.CPU)+" "+inputfilename
            cmd+=" "+outputfilename
            cmd+=" ILLUMINACLIP:TruSeq3-SE.fa:2:30:10 2> "
            cmd+=errorfilename
            #print(cmd)
            os.system(cmd)
            with logging_mutex:
                logger_proxy.info("Trimmomatic run for "+filename+" completed")
            #logger.info('Trimmomatic run for '+filename+" completed")
 
def runRemoveNFromFastqParallel(inputs):
    """
    Parallel implementation of removeNFromFastq
    """
    filename,outputfilename,logger_proxy,logging_mutex=inputs
    fhr=open(filename,"r")
    fhw=open(outputfilename,"w")
    num_seq_with_N=0
    while True:
        line=fhr.readline().strip()
        if not line:break
        seq=fhr.readline().strip()
        useless=fhr.readline().strip()
        quality=fhr.readline().strip()
        #print(line_num)
        if seq[0]=="N" or seq[-1]=="N":
            rseq=seq.rstrip("N")
            right_trim=len(seq)-len(rseq)
            if right_trim!=0:
                quality=quality[:-right_trim]
            lseq=rseq.lstrip("N")
            left_trim=len(rseq)-len(lseq)
            quality=quality[left_trim:]
            if len(lseq)>0:
                fhw.write(line+"\n")
                fhw.write(lseq+"\n")
                fhw.write(useless+"\n")
                fhw.write(quality+"\n")
                num_seq_with_N+=1
        else:
            if len(seq)>0:
                fhw.write(line+"\n")
                fhw.write(seq+"\n")
                fhw.write(useless+"\n")
                fhw.write(quality+"\n")
    fhw.close()
    fhr.close()
    with logging_mutex:
        logger_proxy.info("Removal of flanked Ns for "+filename+" completed")
    #logger.info("Removal of flanked Ns for "+filename+" completed")
    #open(outputfilename.split(".fastq")[0]+".error","w").write(str(num_seq_with_N))

def removeNFromFastq(options,logger_proxy,logging_mutex):
    """
    Reads in the raw data files and remove leading and trailing Ns.
    """    
    #install_mp_handler()
    pool = multiprocessing.Pool(processes=int(options.CPU))
    allinputs=[]
    for num,eachtype in enumerate([options.selected_sample,options.background_sample]):
        for file_num,filename in enumerate(eachtype):
            if num==0:
                inputfilename=options.selected_sample_adapter_trimmed[file_num]
                outputfilename=options.selected_sample_N_removed[file_num]
            else:
                inputfilename=options.background_sample_adapter_trimmed[file_num]
                outputfilename=options.background_sample_N_removed[file_num]
            allinputs.append([inputfilename,outputfilename,logger_proxy,logging_mutex])
    pool.map(runRemoveNFromFastqParallel,allinputs)

def alignReadsWithStarForTrimming(options,logger_proxy,logging_mutex):
    """
    Aligns the untrimmed reads to genome and also generates the 
    mappings to transcriptome
    """
    """for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if file_num%2==1:
                print(num,file_num,eachtype[file_num-1],filename)
    return"""
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            #filename=filename.split("/")[-1].split(".fastq")[0]
            cmd="STAR "
            cmd+=" --runThreadN "+str(options.CPU)+" "
            cmd+=" --genomeDir "+options.star_genome_index
            cmd+=" --genomeLoad LoadAndKeep "
            if options.selected_ended=="SE" and options.background_ended=="SE":
                cmd+=" --readFilesIn "+filename
            else:
                if file_num%2==1:
                    #print(num,file_num,eachtype[file_num-1],filename)
                    cmd+=" --readFilesIn "+eachtype[file_num-1]+" "+filename
                else:
                    continue
            if num==0:
                cmd+=" --outFileNamePrefix "+options.selected_sample_STAR_prefix_round1[file_num]
            else:
                cmd+=" --outFileNamePrefix "+options.background_sample_STAR_prefix_round1[file_num]
            cmd+=" --outSAMtype SAM "
            #cmd+=" --outReadsUnmapped Fastx "
            cmd+=" --outFilterMultimapNmax 500 "
            cmd+=" --outFilterScoreMinOverLread 0.30 --outFilterMatchNminOverLread 0.30 "
            cmd+=" --alignIntronMax 10000 "
            cmd+=" --quantMode TranscriptomeSAM "
            cmd+=" --quantTranscriptomeBan Singleend "
            #cmd+=" --seedPerWindowNmax 100 "
            #cmd+=" --seedPerReadNmax 2000 "
            if num==0:
                cmd+=" > "+options.selected_sample_STAR_round1_output[file_num]
                cmd+=" 2> "+options.selected_sample_STAR_round1_error[file_num]
            else:
                cmd+=" > "+options.background_sample_STAR_round1_output[file_num]
                cmd+=" 2> "+options.background_sample_STAR_round1_error[file_num]
            os.system(cmd)
            
            if num==0:
                cmd="rm "+options.selected_sample_STAR_prefix_round1[file_num]+"Log.out "
                cmd+=options.selected_sample_STAR_prefix_round1[file_num]+"Log.progress.out "
                cmd+=options.selected_sample_STAR_prefix_round1[file_num]+"SJ.out.tab "
                cmd+=options.selected_sample_STAR_round1_output[file_num]
            else:
                cmd="rm "+options.background_sample_STAR_prefix_round1[file_num]+"Log.out "
                cmd+=options.background_sample_STAR_prefix_round1[file_num]+"Log.progress.out "
                cmd+=options.background_sample_STAR_prefix_round1[file_num]+"SJ.out.tab "
                cmd+=options.background_sample_STAR_round1_output[file_num]
            os.system(cmd)
            with logging_mutex:
                logger_proxy.info("STAR round1 mapping for "+filename+" completed")
            #logger.info("STAR round1 mapping for "+filename+" completed")
    cmd="STAR "
    cmd+=" --genomeLoad Remove "
    cmd+=" --genomeDir "+options.star_genome_index
    os.system(cmd)

def removeRedundantMappingsInParallel(inputs):
    """
    - Removes mappings which maps to both vector and genome
    - Preserves the match which is to the vector and discard the rest
    """
    samfile,options=inputs
    # Select those reads which have at least one mapping to the vectors
    cmd="grep prime_end "+samfile+"|grep -v ^\"@\" | cut -f1 | uniq > "+samfile+".reads_mapped_to_vectors"
    #print(cmd)
    os.system(cmd)
    reads_mapped_to_vectors=set(open(samfile+".reads_mapped_to_vectors","r").read().split("\n"))
    #print(len(reads_mapped_to_vectors),"reads mapped to vectors")
    cmd="rm "+samfile+".reads_mapped_to_vectors"
    os.system(cmd)
    
    # Select the correct mappings
    output=samfile+".temp"
    fhw=open(output,"w")
    fhr=open(samfile,"r")
    for line in fhr:
        if "@"==line[0]:
            fhw.write(line)
            continue
        read_id,flag,chromosome=line.split()[0],line.split()[1],line.split()[2]
        if read_id in reads_mapped_to_vectors:
            if "prime" in chromosome:
                fhw.write(line)
        else:
            if flag!="4":
                fhw.write(line)
    fhw.close()
    fhr.close()
    cmd="mv "+output+" "+samfile
    os.system(cmd)
    
def removeRedundantMappings(options):
    """
    - Runs the removeRedundantMappingsInParallel function for all the samfiles
    """
    pool = multiprocessing.Pool(processes=int(options.CPU))
    allinputs=[]
    
    for num,eachtype in enumerate([options.selected_sample,options.background_sample]):
        for file_num,filename in enumerate(eachtype):
            if num==0:
                inputfilename=options.selected_sample_STAR_genome_filename_round1[file_num]
                #inputfilename=options.selected_sample_STAR_prefix_for_vector_trimming[file_num]+"Aligned.out.sam"
            else:
                inputfilename=options.background_sample_STAR_genome_filename_round1[file_num]
                #inputfilename=options.background_sample_STAR_prefix_for_vector_trimming[file_num]+"Aligned.out.sam"
            allinputs.append([inputfilename,options])
    pool.map(removeRedundantMappingsInParallel,allinputs)

def hamming_distance(s1, s2):
    """
    Return the Hamming distance between equal-length sequences
    """
    if len(s1) != len(s2):
        #print(s1)
        #print(s2)
        #raise ValueError("Undefined for sequences of unequal length")
        return len(s1) if len(s1)>len(s2) else len(s2)
    return sum(ch1 != ch2 for ch1, ch2 in zip(s1, s2))

def extractPlasmidSequences(options):
    """
    Returns a dictionary of plasmid sequences
    """
    return readFastaFile(options.plasmid_sequences)

def attempToFindMatches(vector_seq,read_seq,length):
    """
    """
    i=0
    all_hamming_distances=[]
    while i<len(read_seq)-length:
        if hamming_distance(read_seq[i:i+length],vector_seq)!=0:
            all_hamming_distances.append(hamming_distance(read_seq[i:i+length],vector_seq))
        else:
            #print(vector_seq,read_seq[i:i+length])
            #sys.stdout.flush()
            return i,0
        i+=1
    #print(vector_seq,read_seq[all_hamming_distances.index(min(all_hamming_distances)):all_hamming_distances.index(min(all_hamming_distances))+length])
    if min(all_hamming_distances)>2:return -1,-1
    return all_hamming_distances.index(min(all_hamming_distances)),min(all_hamming_distances)

def rev_comp(seq):
    """
    """
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}
    return "".join(complement[base] for base in seq)[::-1]

def checkForThreePrimeVectorEndClipped(read_seq,three_prime_vector,extract_from,options):
    """
    hamming_distance(read_seq[-extract_from:],vectors["3_prime_end"][:extract_from])
    """
    # Check for overlap matches
    #sequences_to_be_checked=[]
    all_hamming_distances=[]
    i=extract_from
    while i>=int(options.min_trimmed_length):
        #sequences_to_be_checked.append([read_seq[-i:],three_prime_vector[:i]])
        all_hamming_distances.append(hamming_distance(read_seq[-i:],three_prime_vector[:i] ))
        all_hamming_distances.append(hamming_distance(read_seq[-i:],reverseComplement(three_prime_vector[:i]) ))
        i-=1
    return min(all_hamming_distances),all_hamming_distances.index(min(all_hamming_distances))%2

def checkForThreePrimeVectorFrontClipped(read_seq,three_prime_vector,extract_till,options):
    """
    hamming_distance(read_seq[-extract_from:],vectors["3_prime_end"][:extract_from])
    """
    # Check for overlap matches
    #sequences_to_be_checked=[]
    all_hamming_distances=[]
    i=extract_till
    while i>=int(options.min_trimmed_length):
        #sequences_to_be_checked.append([read_seq[-i:],three_prime_vector[:i]])
        all_hamming_distances.append(hamming_distance(read_seq[:i],three_prime_vector[:i] ))
        all_hamming_distances.append(hamming_distance(read_seq[:i],reverseComplement(three_prime_vector[:i]) ))
        i-=1
    return min(all_hamming_distances),all_hamming_distances.index(min(all_hamming_distances))%2

def reverseComplement(seq):
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}
    return "".join([complement[base] for base in seq])[::-1]
    
def findVectorInReadPrefix(read_id,read_seq,vector,options,start,end):
    """
    """
    # Check for presence of vector in the 5 prime end of read_seq
    first_start=start
    all_hamming_distances=[]
    """if "read13530682" in read_id:
        print(read_seq)"""
    while start<=end:
        """if "read13530682" in read_id:
            print(start,vector[-start:],read_seq[:start])"""
        all_hamming_distances.append(hamming_distance(vector[-start:],read_seq[:start]))
        start+=1
    """if "read13530682" in read_id:
        print(all_hamming_distances)"""
    if len(all_hamming_distances)==0:
        return 10,10 # Sending a high value of hamming distance so that this read gets eliminated from being characterized as a fusion read
    return min(all_hamming_distances),all_hamming_distances.index(min(all_hamming_distances))+first_start
    
def findVectorInReadSuffix(read_id,read_seq,vector,options,start,end):
    """
    """
    all_hamming_distances=[]
    first_start=start
    while start<=end:
        #all_hamming_distances.append(hamming_distance(vector[-start:],read_seq[:start]))
        all_hamming_distances.append(hamming_distance(vector[:start],read_seq[-start:]))
        start+=1
    if len(all_hamming_distances)==0:
        return 10,10 # Sending a high value of hamming distance so that this read gets eliminated from being characterized as a fusion read 
    return min(all_hamming_distances),len(read_seq)-all_hamming_distances.index(min(all_hamming_distances))-first_start
    
def checkForVectorsInRead(read_id,read_seq,vectors,cigar,options):
    """
    """
    prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse=5,5,5,5
    max_limit_end=25
    if re.search(r'^\d*S[A-Z0-9]*\d*S$',cigar):
        start=int(options.min_trimmed_length)
        end=((int(cigar.split("S")[0])+max_limit_end) if (int(cigar.split("S")[0]) > start) else int(options.min_trimmed_length)+max_limit_end)
        if int(cigar.split("S")[0])>=int(options.min_trimmed_length):
            prefix_min_hamming_dist_5_prime_forward,prefix_cut_5_prime_forward=findVectorInReadPrefix(read_id,read_seq,vectors["5_prime_end"],options,start,end)
            prefix_cut_5_prime_forward=prefix_cut_5_prime_forward-int(options.frame_of_TF_fusion)
            start,end=int(options.min_trimmed_length),int(0.80*len(read_seq))
            prefix_min_hamming_dist_3_prime_reverse,prefix_cut_3_prime_reverse=findVectorInReadSuffix(read_id,reverseComplement(read_seq),vectors["3_prime_end"],options,start,end)
        #===================================================================================================================================================================
        
        start=int(options.min_trimmed_length)
        end=([int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1] +max_limit_end) if ([int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1] > start) else (int(options.min_trimmed_length) +max_limit_end)
        if [int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1]>=int(options.min_trimmed_length):
            prefix_min_hamming_dist_5_prime_reverse,prefix_cut_5_prime_reverse=findVectorInReadPrefix(read_id,reverseComplement(read_seq),vectors["5_prime_end"],options,start,end)
            prefix_cut_5_prime_reverse=prefix_cut_5_prime_reverse-int(options.frame_of_TF_fusion)
            start,end=int(options.min_trimmed_length),int(0.80*len(read_seq))
            prefix_min_hamming_dist_3_prime_forward,prefix_cut_3_prime_forward=findVectorInReadSuffix(read_id,read_seq,vectors["3_prime_end"],options,start,end)
    elif re.search(r'\d*S$',cigar):
        start=int(options.min_trimmed_length)
        end=([int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1] +max_limit_end) if ([int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1] > start) else (int(options.min_trimmed_length) +max_limit_end)
        if [int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1]>=int(options.min_trimmed_length):
            prefix_min_hamming_dist_5_prime_reverse,prefix_cut_5_prime_reverse=findVectorInReadPrefix(read_id,reverseComplement(read_seq),vectors["5_prime_end"],options,start,end)
            prefix_cut_5_prime_reverse=prefix_cut_5_prime_reverse-int(options.frame_of_TF_fusion)
            start,end=int(options.min_trimmed_length),int(0.80*len(read_seq))
            prefix_min_hamming_dist_3_prime_forward,prefix_cut_3_prime_forward=findVectorInReadSuffix(read_id,read_seq,vectors["3_prime_end"],options,start,end)    
            prefix_min_hamming_dist_3_prime_reverse,prefix_min_hamming_dist_5_prime_forward=1000,1000 # Assigning dummy values
            
    elif re.search(r'^\d*S',cigar):
        start=int(options.min_trimmed_length)
        end=((int(cigar.split("S")[0])+max_limit_end) if (int(cigar.split("S")[0]) > start) else int(options.min_trimmed_length)+max_limit_end)
        """if "read13530682" in read_id:
            print("startend",start,end)"""
        if int(cigar.split("S")[0])>=int(options.min_trimmed_length):
            prefix_min_hamming_dist_5_prime_forward,prefix_cut_5_prime_forward=findVectorInReadPrefix(read_id,read_seq,vectors["5_prime_end"],options,start,end)
            prefix_cut_5_prime_forward=prefix_cut_5_prime_forward-int(options.frame_of_TF_fusion)
            start,end=int(options.min_trimmed_length),int(0.80*len(read_seq))
            prefix_min_hamming_dist_3_prime_reverse,prefix_cut_3_prime_reverse=findVectorInReadSuffix(read_id,reverseComplement(read_seq),vectors["3_prime_end"],options,start,end)
            prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_5_prime_reverse=1000,1000 # Assigning dummy values
        
    """if "read13530682" in read_id:
        print(read_id)
        print(cigar)
        print(start,end)
        print(prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse)
        sys.stdout.flush()"""
    if min([prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse])>5:
        return -1,-1,-1
    
    
    if prefix_min_hamming_dist_5_prime_forward<min([prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse]):
        """print("5_prime_end_forward")
        print(cigar)
        print(read_seq)
        print("-"*prefix_cut_5_prime_forward+read_seq[prefix_cut_5_prime_forward:])
        print(prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse)
        print("="*200)
        """
        return prefix_cut_5_prime_forward+1,len(read_seq),"5_prime_end_forward"
        pass
    elif prefix_min_hamming_dist_5_prime_reverse<min([prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse]):
        """print("5_prime_end_reverse")
        print(cigar)
        print(read_seq)
        print(read_seq[:-prefix_cut_5_prime_reverse]+"-"*prefix_cut_5_prime_reverse)
        print(prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse)
        print("="*200)
        """
        return 1,len(read_seq)-prefix_cut_5_prime_reverse,"5_prime_end_reverse"
        pass
    elif prefix_min_hamming_dist_3_prime_forward<min([prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_reverse]):
        """print("3_prime_end_forward")
        print(read_id,cigar)
        print(read_seq)
        print(prefix_cut_3_prime_forward)
        print(read_seq[:prefix_cut_3_prime_forward]+"-"*(len(read_seq)-prefix_cut_3_prime_forward))
        print(prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse)
        print(1,prefix_cut_3_prime_forward,"3_prime_end_forward")
        print("="*200)"""
        return 1,prefix_cut_3_prime_forward,"3_prime_end_forward"
        pass
    elif prefix_min_hamming_dist_3_prime_reverse<min([prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward]):
        """print("3_prime_end_reverse")
        print(read_id,cigar)
        print(read_seq)
        print(prefix_cut_3_prime_reverse)
        print("-"*(len(read_seq)-prefix_cut_3_prime_reverse)+read_seq[-prefix_cut_3_prime_reverse:])
        print(prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse)
        
        print(len(read_seq)-prefix_cut_3_prime_reverse+1,len(read_seq),"3_prime_end_reverse")
        print("="*200)"""
        return len(read_seq)-prefix_cut_3_prime_reverse+1,len(read_seq),"3_prime_end_reverse"
        pass
    else:
        """print("MATCHES NOTHING")
        print(prefix_min_hamming_dist_5_prime_forward,prefix_min_hamming_dist_5_prime_reverse,prefix_min_hamming_dist_3_prime_forward,prefix_min_hamming_dist_3_prime_reverse)
        print("="*200)"""
        return -1,-1,-1
    
def findReadsWithVectorSequence(inputs):
    """
    Finds and flags the reads with vector sequence
    """
    vectors,aligned_filename,filename,options,trimming_stats_filename,logfile,logger_proxy,logging_mutex=inputs
    
    reads_discarded={"whole_5_prime_vector":[],
                     "whole_3_prime_vector":[],
                     "incorrect_fragment_amplified_5_prime_vector":[],
                     "incorrect_fragment_amplified_3_prime_vector":[],
                     "polyAtail":[],
                     "too_small_usable_fragment":[],
                     "other":[]}

    five_prime_trim_read_info={}
    three_prime_trim_read_info={}    
    hits_to_yeast_chromomosome={}
    hits_to_yeast_plasmid={}
    
    plasmids=extractPlasmidSequences(options)
    complement = {'A': 'T', 'C': 'G', 'G': 'C', 'T': 'A', 'N': 'N'}
    
    fhw=open(logfile,"w")
    fhw.write("Read_id,explanation,retained_start,retained_end\n")
    
    fhr=open(aligned_filename,"r")
    for line in fhr:
        if line[0]=="@":continue
        line=line.strip()
        #print(line)
        read_id,mapping_orientation_STAR,chromosome,map_pos,score,cigar,useless1,useless2,useless3,read_seq,read_qual=line.split("\t")[:11]
        if options.selected_ended=="PE" and options.background_ended=="PE":
            read_id=read_id+"_"+("1" if isKthBitSet(int(mapping_orientation_STAR),7)==True else "2")
        mapping_orientation="0" if isKthBitSet(int(mapping_orientation_STAR),5)==False else "1"
        # Skipping reads that do not have any soft clips
        if "S" not in cigar:continue
        #if "prime" in chromosome:continue
        if read_id in five_prime_trim_read_info or read_id in three_prime_trim_read_info:continue
        if "prime" in chromosome:
            """if "read14913735" in read_id:
                print(re.search(r'^\d*M[MID0-9]*\d*M$',cigar) )
                print(re.search(r'^\d*M$',cigar))
                sys.stdout.flush()"""
            if re.search(r'^\d*M[MID0-9]*\d*M$',cigar) or re.search(r'^\d*M$',cigar):
                """if "read14913735" in read_id:
                    print("I am inside")
                    sys.stdout.flush()"""
                # Matches cigar of the form 149M --> Matches the whole Vector
                if chromosome=="5_prime_end":
                    reads_discarded["whole_5_prime_vector"].append(read_id)
                    fhw.write(read_id+",whole_5_prime_vector,-1,-1"+"\n")
                else:
                    reads_discarded["whole_3_prime_vector"].append(read_id)
                    fhw.write(read_id+",whole_3_prime_vector,-1,-1"+"\n")
            
            elif re.search(r'^\d*S[a-zA-Z0-9]*\d*S$',cigar) or re.search(r'^(\d*[MID])*\d*S$',cigar):
                # Matches cigar of the form 34S*3S
                if chromosome=="5_prime_end":
                    extract_from=[int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1]
                    if extract_from < options.min_trimmed_length:
                        reads_discarded["too_small_usable_fragment"].append(read_id)
                        fhw.write(read_id+",too_small_fragment_5_prime_vector,-1,-1"+"\n")
                        continue
                    retain_from=len(read_seq)-(extract_from+options.frame_of_TF_fusion-1)
                    retain_to=len(read_seq)
                    five_prime_trim_read_info[read_id]={"retain_from":retain_from,
                                                        "retain_to":retain_to,
                                                        "vector":"5_prime_end_forward" if mapping_orientation=="0" else "5_prime_end_reverse",
                                                        "seq":read_seq[retain_from-1:retain_to] if mapping_orientation=="0" else reverseComplement(read_seq[retain_from-1:retain_to]),
                                                        "qual":read_qual[retain_from-1:retain_to] if mapping_orientation=="0" else read_qual[retain_from-1:retain_to][::-1],
                                                        "cigar":cigar}
                    """if five_prime_trim_read_info[read_id]["seq"][:2]!="GG" and five_prime_trim_read_info[read_id]["seq"][-2:]!="CC":
                        #print(dist_from_5_prime_vector,dist_from_3_prime_vector)
                        print(read_id,five_prime_trim_read_info[read_id]["vector"])
                        print(read_seq)
                        print(retain_from,retain_to)
                        print(five_prime_trim_read_info[read_id]["seq"])
                        print("="*150) 
                        sys.stdout.flush()"""
                    fhw.write(read_id+(",5_prime_end_forward," if mapping_orientation=="0" else ",5_prime_end_reverse,")+str(retain_from)+","+str(retain_to)+"\n")
                else:
                    extract_till=[int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][0]
                    if extract_till < options.min_trimmed_length:
                        reads_discarded["too_small_usable_fragment"].append(read_id)
                        #fhw.write(read_id+",too_small_fragment_3_prime_vector,-1,-1"+"\n")
                        continue
                    seq_trimmed=read_seq[:extract_till]
                    if seq_trimmed.count('A')/extract_till>0.9 or seq_trimmed.count('T')/extract_till>0.9:
                        reads_discarded["polyAtail"].append(read_id) 
                        #fhw.write(read_id+",polyA_tail,-1,-1"+"\n")
                        continue
                    three_prime_trim_read_info[read_id]={"retain_from":1,
                                                         "retain_to":extract_till,
                                                         "vector":"3_prime_end_forward" if mapping_orientation=="0" else "3_prime_end_reverse",
                                                         "seq":read_seq[:extract_till] if mapping_orientation=="0" else reverseComplement(read_seq[:extract_till]),
                                                         "qual":read_qual[:extract_till] if mapping_orientation=="0" else read_qual[:extract_till][::-1],
                                                         "cigar":cigar}
                    fhw.write(read_id+(",3_prime_end_forward," if mapping_orientation=="0" else ",3_prime_end_reverse,")+str(1)+","+str(extract_till)+"\n")
                    
            elif re.search(r'\d*S$',cigar):
                # Matches cigar of form 130M30S
                if chromosome=="5_prime_end":
                    extract_from=[int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][-1]
                    if extract_from < options.min_trimmed_length:
                        reads_discarded["too_small_usable_fragment"].append(read_id)
                        fhw.write(read_id+",too_small_fragment_5_prime_vector,-1,-1"+"\n")
                        continue
                    retain_from=len(read_seq)-(extract_from+options.frame_of_TF_fusion-1)
                    retain_to=len(read_seq)
                    five_prime_trim_read_info[read_id]={"retain_from":retain_from,
                                                        "retain_to":retain_to,
                                                        "vector":"5_prime_end_forward" if mapping_orientation=="0" else "5_prime_end_reverse",
                                                        "seq":read_seq[retain_from-1:retain_to] if mapping_orientation=="0" else reverseComplement(read_seq[retain_from-1:retain_to]),
                                                        "qual":read_qual[retain_from-1:retain_to] if mapping_orientation=="0" else read_qual[retain_from-1:retain_to][::-1],
                                                        "cigar":cigar}
                    """if five_prime_trim_read_info[read_id]["seq"][:2]!="GG" and five_prime_trim_read_info[read_id]["seq"][-2:]!="CC":
                        #print(dist_from_5_prime_vector,dist_from_3_prime_vector)
                        print(read_id,five_prime_trim_read_info[read_id]["vector"])
                        print(read_seq)
                        print(retain_from,retain_to)
                        print(five_prime_trim_read_info[read_id]["seq"])
                        print("="*150) 
                        sys.stdout.flush()"""
                    fhw.write(read_id+(",5_prime_end_forward," if mapping_orientation=="0" else ",5_prime_end_reverse,")+str(retain_from)+","+str(retain_to)+"\n")
                elif chromosome=="3_prime_end":
                    reads_discarded["incorrect_fragment_amplified_3_prime_vector"].append(read_id)
                    fhw.write(read_id+",incorrect_fragment_amplified_3_prime_vector,-1,-1"+"\n")
            elif re.search(r'^\d*S',cigar):
                # Matches cigar of the form 30S120M
                if chromosome=="3_prime_end":
                    extract_till=int(cigar.split("S")[0])
                    if extract_till < options.min_trimmed_length:
                        reads_discarded["too_small_usable_fragment"].append(read_id)
                        fhw.write(read_id+",too_small_fragment_3_prime_vector,-1,-1"+"\n")
                        continue
                    seq_trimmed=read_seq[:extract_till]
                    if seq_trimmed.count('A')/extract_till>0.9 or seq_trimmed.count('T')/extract_till>0.9:
                        reads_discarded["polyAtail"].append(read_id) 
                        fhw.write(read_id+",polyA_tail,-1,-1"+"\n")
                        continue
                    three_prime_trim_read_info[read_id]={"retain_from":1,
                                                         "retain_to":extract_till,
                                                         "vector":"3_prime_end_forward" if mapping_orientation=="0" else "3_prime_end_reverse",
                                                         "seq":read_seq[:extract_till] if mapping_orientation=="0" else reverseComplement(read_seq[:extract_till]),
                                                        "qual":read_qual[:extract_till] if mapping_orientation=="0" else read_qual[:extract_till][::-1],
                                                         "cigar":cigar}
                    fhw.write(read_id+(",3_prime_end_forward," if mapping_orientation=="0" else ",3_prime_end_reverse,")+str(1)+","+str(extract_till)+"\n")
                elif chromosome=="5_prime_end":
                    reads_discarded["incorrect_fragment_amplified_5_prime_vector"].append(read_id)
                    fhw.write(read_id+",incorrect_fragment_amplified_5_prime_vector,-1,-1"+"\n")
            else:
                if chromosome=="5_prime_end":
                    reads_discarded["other"].append(read_id)
                    fhw.write(read_id+",other_5_prime_vector,-1,-1"+"\n")
                else:
                    reads_discarded["other"].append(read_id)
                    fhw.write(read_id+",other_3_prime_vector,-1,-1"+"\n")
        elif chromosome in plasmids.keys():
            hits_to_yeast_plasmid[read_id]={"retain_from":0,"retain_to":-1,"seq":read_seq,"qual":read_qual,"cigar":cigar}
            fhw.write(read_id+",plasmid,-1,-1"+"\n")
        else:
            # Most of the read contains transcriptomic sequence
            
            # Inverting the read, the quality values and the CIGAR if the mapping orientation is reversed
            if isKthBitSet(int(mapping_orientation_STAR),5)==True:
                read_seq="".join(complement[base] for base in read_seq)[::-1]
                read_qual=read_qual[::-1]
                cigar_values=[int(s) for s in re.findall(r'\d+',cigar)][::-1]
                cigar_alphabets=re.findall(r'[A-Z]',cigar)
                cigar_alphabets=cigar_alphabets[::-1]
                cigar=""
                for j in range(len(cigar_values)):
                    cigar+=str(cigar_values[j])+cigar_alphabets[j]
            
            retain_from,retain_to,vector=checkForVectorsInRead(read_id,read_seq,vectors,cigar,options)
            if vector==-1:
                continue
            if "5_prime" in vector:
                five_prime_trim_read_info[read_id]={"retain_from":retain_from,
                                                        "retain_to":retain_to,
                                                        "vector":vector,
                                                        "seq":read_seq[retain_from-1:retain_to] ,
                                                        "qual":read_qual[retain_from-1:retain_to],
                                                        "cigar":cigar}
            else:
                three_prime_trim_read_info[read_id]={"retain_from":retain_from,
                                                        "retain_to":retain_to,
                                                        "vector":vector,
                                                        "seq":read_seq[retain_from-1:retain_to] ,
                                                        "qual":read_qual[retain_from-1:retain_to],
                                                        "cigar":cigar}
            fhw.write(read_id+","+vector+","+str(retain_from)+","+str(retain_to)+"\n")
        #print(read_id)
                    
    fhr.close()
    #fhw.close()
    #print("reads_not_trimmed_due_to_small_vector_sequence",len(reads_not_trimmed_due_to_small_vector_sequence))
    length_distros=[[],[],[]]
    fhw=open(filename+"_trimming_info.pkl","wb")
    pickle.dump([filename,length_distros,five_prime_trim_read_info,three_prime_trim_read_info,hits_to_yeast_chromomosome,hits_to_yeast_plasmid,reads_discarded],fhw)
    fhw.close()
    
    fhw=open(trimming_stats_filename,"w")
    five_prime_forward,five_prime_reverse,three_prime_forward,three_prime_reverse=0,0,0,0
    for read_id in five_prime_trim_read_info:
        if "forward" in five_prime_trim_read_info[read_id]["vector"]:
            five_prime_forward+=1
        elif "reverse" in five_prime_trim_read_info[read_id]["vector"]:
            five_prime_reverse+=1
            
    for read_id in three_prime_trim_read_info:
        if "forward" in three_prime_trim_read_info[read_id]["vector"]:
            three_prime_forward+=1
        elif "reverse" in three_prime_trim_read_info[read_id]["vector"]:
            three_prime_reverse+=1
    
    fhw.write("Five Prime Forward "+str(five_prime_forward)+"\n")
    fhw.write("Five Prime Reverse "+str(five_prime_reverse)+"\n")
    fhw.write("Five Prime Vector whole read "+str(len(reads_discarded["whole_5_prime_vector"]))+"\n")
    fhw.write("Three Prime Forward "+str(three_prime_forward)+"\n")
    fhw.write("Three Prime Reverse "+str(three_prime_reverse)+"\n")
    fhw.write("Three Prime Vector whole read "+str(len(reads_discarded["whole_3_prime_vector"]))+"\n")
    fhw.write("Plasmid "+str(len(hits_to_yeast_plasmid))+"\n")
    fhw.write("Yeast Chromosome "+str(len(hits_to_yeast_chromomosome))+"\n")
    fhw.write("Reads removed due to small size "+str(len(reads_discarded["too_small_usable_fragment"]))+"\n")
    fhw.write("Reads removed due to polyA "+str(len(reads_discarded["polyAtail"]))+"\n")
    fhw.write("num_junction_reads "+str(len(five_prime_trim_read_info)+len(three_prime_trim_read_info))+"\n")
    #fhw.write("num_all_reads ",str(len(reads_discarded[""])))
    fhw.close()
    
    with logging_mutex:
        logger_proxy.info("Finding fusion reads for "+filename+" completed")

    """print(trimming_stats_filename)
    for key in reads_discarded:
        print(key,len(reads_discarded[key]))
    print("Five prime vectors",len(five_prime_trim_read_info))
    print("Three prime vectors",len(three_prime_trim_read_info))"""
    #logger.info("Finding fusion reads for "+filename+" completed")   

def trimVectorFromReadsSingleEnded(inputs):
    """
    Trim vector off reads and stores the results in two different files
    """
    filename,options,reads_to_be_trimmed,trimmed_reads_filename,junction_reads_filename,trimming_stats_filename,reads_discarded=inputs
    fhr=open(filename,"r")
    fhw_trim=open(trimmed_reads_filename,"w")
    fhw_junction=open(junction_reads_filename,"w")
    #fhw_untrimmed_reads=open(filename+"_untrimmed_reads.txt","w")
    reads_to_be_discarded=[]
    for key in reads_discarded:
        reads_to_be_discarded.extend(reads_discarded[key])
    """print(filename,len(reads_to_be_trimmed),len(reads_to_be_discarded))
    sys.stdout.flush()
    return"""
    reads_to_be_discarded=set(reads_to_be_discarded)
    while True:
        line=fhr.readline()
        if not line:break
        if line.strip().split()[0][1:] in reads_to_be_trimmed:
            fhr.readline()
            useless=fhr.readline().strip()
            fhr.readline()
            read_id=line.strip().split()[0][1:]
            retain_from=reads_to_be_trimmed[read_id]["retain_from"]
            retain_to=reads_to_be_trimmed[read_id]["retain_to"]
            
            vector=reads_to_be_trimmed[read_id]["vector"]
            seq=reads_to_be_trimmed[read_id]["seq"]
            qual=reads_to_be_trimmed[read_id]["qual"]
            cigar=reads_to_be_trimmed[read_id]["cigar"]
            
            fhw_junction.write("@"+read_id+"_"+vector+"_"+str(retain_from)+"_"+str(retain_to)+"\n")
            fhw_junction.write(seq+"\n")
            fhw_junction.write(useless+"\n")
            fhw_junction.write(qual+"\n")
            
            fhw_trim.write("@"+read_id+"_"+vector+"_"+str(retain_from)+"_"+str(retain_to)+"\n")
            fhw_trim.write(seq+"\n")
            fhw_trim.write(useless+"\n")
            fhw_trim.write(qual+"\n")
        else:
            if line.strip().split()[0][1:] in reads_to_be_discarded:
                #print("Skipping reads discarded")
                #sys.stdout.flush()
                fhr.readline()
                fhr.readline()
                fhr.readline()
                continue
            fhw_trim.write("@"+line.strip().split()[0][1:]+"\n")
            #fhw_untrimmed_reads.write(line.strip().split()[0][1:]+"\n")
            fhw_trim.write(fhr.readline())
            fhw_trim.write(fhr.readline())
            fhw_trim.write(fhr.readline())
            
    fhr.close()
    fhw_trim.close()
    fhw_junction.close()
    #logger.info("Fusion reads written to "+junction_reads_filename)
    #fhw_untrimmed_reads.close()
    """open(trimming_stats_filename,"a+").write("Reads removed due to small size "+str(reads_removed_small_size)+"\n")
    open(trimming_stats_filename,"a+").write("Reads removed due to polyA "+str(reads_removed_polyA_tail)+"\n")
    open(trimming_stats_filename,"a+").write("num_junction_reads "+str(num_junction_reads)+"\n")
    open(trimming_stats_filename,"a+").write("num_all_reads "+str(num_all_reads)+"\n")"""
    
def trimVectorFromReadsPairedEnded(inputs):
    """
    Trim vector off reads and stores the results in two different files
    """
    filename_mate1,filename_mate2,options,reads_to_be_trimmed,trimmed_reads_filename_mate1,trimmed_reads_filename_mate2,junction_reads_filename_mate1,junction_reads_filename_mate2,trimming_stats_filename,reads_discarded=inputs
    """print("\n".join([filename_mate1,filename_mate2,trimmed_reads_filename_mate1,trimmed_reads_filename_mate2,junction_reads_filename_mate1,junction_reads_filename_mate2,trimming_stats_filename]))
    print("="*150)
    sys.stdout.flush()
    """
    fhr_mate1=open(filename_mate1,"r")
    fhr_mate2=open(filename_mate2,"r")
    fhw_trim_mate1=open(trimmed_reads_filename_mate1,"w")
    fhw_trim_mate2=open(trimmed_reads_filename_mate2,"w")
    fhw_junction_mate1=open(junction_reads_filename_mate1,"w")
    fhw_junction_mate2=open(junction_reads_filename_mate2,"w")
    
    reads_to_be_discarded=[]
    for key in reads_discarded:
        reads_to_be_discarded.extend(reads_discarded[key])
        
    reads_to_be_discarded=set(reads_to_be_discarded)
    """print(filename_mate1,len(reads_to_be_discarded),len(reads_to_be_trimmed))
    sys.stdout.flush()"""
    while True:
        line_mate1=fhr_mate1.readline()
        line_mate2=fhr_mate2.readline()
        if (not line_mate1) or (not line_mate2):break
        if line_mate1.strip().split()[0][1:]+"_1" in reads_to_be_trimmed or line_mate2.strip().split()[0][1:]+"_2" in reads_to_be_trimmed:
            #print("Inside here",line_mate1,line_mate2)
            """fhr_mate1.readline()
            fhr_mate1.readline()
            fhr_mate1.readline()
            
            fhr_mate2.readline()
            fhr_mate2.readline()
            fhr_mate2.readline()
            continue"""
            # progress the file pointers to the next entry in both files
            seq_mate1=fhr_mate1.readline().strip()
            useless_mate1=fhr_mate1.readline().strip()
            qual_mate1=fhr_mate1.readline().strip()
            seq_mate2=fhr_mate2.readline().strip()
            useless_mate2=fhr_mate2.readline().strip()
            qual_mate2=fhr_mate2.readline().strip()
            
            read_id_mate1,retain_from_mate1,retain_to_mate1,read_id_mate2,retain_from_mate2,retain_to_mate2="","","","","",""
            if line_mate1.strip().split()[0][1:]+"_1" in reads_to_be_trimmed:
                read_id_mate1=line_mate1.strip().split()[0][1:]+"_1"
                retain_from_mate1=reads_to_be_trimmed[read_id_mate1]["retain_from"]
                retain_to_mate1=reads_to_be_trimmed[read_id_mate1]["retain_to"]
                
            if line_mate2.strip().split()[0][1:]+"_2" in reads_to_be_trimmed:
                read_id_mate2=line_mate2.strip().split()[0][1:]+"_2"
                retain_from_mate2=reads_to_be_trimmed[read_id_mate2]["retain_from"]
                retain_to_mate2=reads_to_be_trimmed[read_id_mate2]["retain_to"]
            
            if (read_id_mate1=="" and read_id_mate2!="") or (read_id_mate1!="" and read_id_mate2==""):
                # Vector sequence present in either read_id_mate1 or read_id_mate2 
                if read_id_mate1!="" and read_id_mate2=="":
                    read_id,retain_from,retain_to=read_id_mate1,retain_from_mate1,retain_to_mate1
                    fhw_junction=fhw_junction_mate1
                    fhw_trim=fhw_trim_mate1
                    useless=useless_mate1
                elif read_id_mate1=="" and read_id_mate2!="":
                    read_id,retain_from,retain_to=read_id_mate2,retain_from_mate2,retain_to_mate2
                    fhw_junction=fhw_junction_mate2
                    fhw_trim=fhw_trim_mate2
                    useless=useless_mate2
                
                vector=reads_to_be_trimmed[read_id]["vector"]
                seq=reads_to_be_trimmed[read_id]["seq"]
                qual=reads_to_be_trimmed[read_id]["qual"]
                cigar=reads_to_be_trimmed[read_id]["cigar"]
                read_id=read_id[:-2]
                read_header="@"+read_id+"_"+vector+"_"+str(retain_from)+"_"+str(retain_to)
                
                fhw_junction.write(read_header+"\n")
                fhw_trim.write("@"+read_id+"_"+vector+"_"+str(retain_from)+"_"+str(retain_to)+"\n")
                fhw_junction.write(seq+"\n")
                fhw_trim.write(seq+"\n")             
                fhw_junction.write(useless+"\n")
                fhw_trim.write(useless+"\n")
                fhw_junction.write(qual+"\n")
                fhw_trim.write(qual+"\n")
                
                if read_id_mate1!="" and read_id_mate2=="":
                    fhw_junction=fhw_junction_mate2
                    fhw_trim=fhw_trim_mate2
                    useless=useless_mate2
                    line=line_mate2
                    fhr=fhr_mate2
                    seq=seq_mate2
                    qual=qual_mate2
                else:
                    fhw_junction=fhw_junction_mate1
                    fhw_trim=fhw_trim_mate1
                    useless=useless_mate1
                    line=line_mate1
                    fhr=fhr_mate1
                    seq=seq_mate1
                    qual=qual_mate1
                fhw_trim.write(read_header+"\n")
                fhw_trim.write(seq+"\n")
                fhw_trim.write(useless+"\n")
                fhw_trim.write(qual+"\n")
                
                fhw_junction.write(read_header+"\n")
                fhw_junction.write(seq+"\n")
                fhw_junction.write(useless+"\n")
                fhw_junction.write(qual+"\n")
            else:
                # Vector sequence present in both read_id_mate1 and read_id_mate2
                for entry_num,each_entry in enumerate([[read_id_mate1,retain_from_mate1,retain_to_mate1],[read_id_mate2,retain_from_mate2,retain_to_mate2]]):
                    read_id,retain_from,retain_to=each_entry
                    vector=reads_to_be_trimmed[read_id]["vector"]
                    seq=reads_to_be_trimmed[read_id]["seq"]
                    qual=reads_to_be_trimmed[read_id]["qual"]
                    cigar=reads_to_be_trimmed[read_id]["cigar"]
                    
                    read_id=read_id[:-2]
                    if entry_num==0:
                        fhw_junction=fhw_junction_mate1
                        fhw_trim=fhw_trim_mate1
                        useless=useless_mate1
                    else:
                        fhw_junction=fhw_junction_mate2
                        fhw_trim=fhw_trim_mate2
                        useless=useless_mate2
                    
                    fhw_junction.write("@"+read_id+"_"+vector+"_"+str(retain_from)+"_"+str(retain_to)+"\n")
                    fhw_trim.write("@"+read_id+"_"+vector+"_"+str(retain_from)+"_"+str(retain_to)+"\n")
        
                    fhw_junction.write(seq+"\n")
                    fhw_trim.write(seq+"\n")
                    fhw_junction.write(useless+"\n")
                    fhw_trim.write(useless+"\n")
                    fhw_junction.write(qual+"\n")
                    fhw_trim.write(qual+"\n")
        else:
            for entry_num,each_entry in enumerate([[fhr_mate1,fhw_junction_mate1,fhw_trim_mate1,line_mate1],[fhr_mate2,fhw_junction_mate2,fhw_trim_mate2,line_mate2]]):
                fhr,fhw_junction,fhw_trim,line=each_entry
                #print(line)
                fhw_trim.write("@"+line.strip().split()[0][1:]+"\n")
                fhw_trim.write(fhr.readline())
                fhw_trim.write(fhr.readline())
                fhw_trim.write(fhr.readline())
            """fhw_trim_mate1.write("@"+line_mate1.strip().split()[0][1:]+"\n")
            fhw_trim_mate1.write(fhr_mate1.readline())
            fhw_trim_mate1.write(fhr_mate1.readline())
            fhw_trim_mate1.write(fhr_mate1.readline())   
        
            fhw_trim_mate2.write("@"+line_mate2.strip().split()[0][1:]+"\n")
            fhw_trim_mate2.write(fhr_mate2.readline())
            fhw_trim_mate2.write(fhr_mate2.readline())
            fhw_trim_mate2.write(fhr_mate2.readline())  """  
            
    fhr_mate1.close()
    fhr_mate2.close()
    fhw_trim_mate1.close()
    fhw_trim_mate2.close()
    fhw_junction_mate1.close()
    fhw_junction_mate2.close()
    
    #logger.info("Fusion reads written to "+junction_reads_filename_mate1+" and "+junction_reads_filename_mate2)
    """open(trimming_stats_filename,"a+").write("Reads removed due to small size "+str(reads_removed_small_size)+"\n")
    open(trimming_stats_filename,"a+").write("Reads removed due to polyA "+str(reads_removed_polyA_tail)+"\n")
    open(trimming_stats_filename,"a+").write("num_junction_reads "+str(num_junction_reads)+"\n")
    open(trimming_stats_filename,"a+").write("num_all_reads "+str(num_all_reads)+"\n")"""

def findReadsWithVectorSequenceAndTrim(options,logger_proxy,logging_mutex):
    """
    Looks through the STAR alignments to find reads with vector sequence 
    """
    vectors=readFastaFile(options.vector_sequences)
    pool = multiprocessing.Pool(processes=int(options.CPU))
    allinputs=[]
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            if num==0:
                aligned_filename=options.selected_sample_STAR_genome_filename_round1[file_num]
                trimming_stats_filename=options.selected_sample_trimming_stats[file_num]
                logfile=options.selected_sample_per_read_log[file_num]
            else:
                aligned_filename=options.background_sample_STAR_genome_filename_round1[file_num]
                trimming_stats_filename=options.background_sample_trimming_stats[file_num]
                logfile=options.background_sample_per_read_log[file_num]
            allinputs.append([vectors,aligned_filename,filename,options,trimming_stats_filename,logfile,logger_proxy,logging_mutex])
    #pool.map(findReadsWithVectorSequence,allinputs)
    pool.map(findReadsWithVectorSequence,allinputs)
    
    results=[]
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            results.append(pickle.load(open(filename+"_trimming_info.pkl","rb")))
    
    #print(results)
    results_dict={}
    for result in results:
        results_dict[result[0]]={"lengths_all_trimmed_reads":result[1][0],
                                 "lengths_all_5_prime_trimmed_reads":result[1][1],
                                 "lengths_all_3_prime_trimmed_reads":result[1][2],
                                 "five_prime_trim_read_info":result[2],
                                 "three_prime_trim_read_info":result[3],
                                 "hits_to_yeast_chromomosome":result[4],
                                 "hits_to_yeast_plasmid":result[5],
                                 "reads_discarded":result[6]
                                 }
        """print("Reading",result[0],"completed")
        sys.stdout.flush()"""
        """print(result[0],len(results_dict[result[0]]["five_prime_trim_read_info"])+len(results_dict[result[0]]["three_prime_trim_read_info"]))
        sys.stdout.flush()"""
    # Clean up the reads and create new files
    pool = multiprocessing.Pool(processes=int(options.CPU)-1)
    allinputs=[]
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            reads_to_be_trimmed={}
            reads_to_be_trimmed.update(results_dict[filename]["five_prime_trim_read_info"])
            reads_to_be_trimmed.update(results_dict[filename]["three_prime_trim_read_info"])
            #reads_to_be_trimmed.update(results_dict[filename]["hits_to_yeast_chromomosome"])
            #reads_to_be_trimmed.update(results_dict[filename]["hits_to_yeast_plasmid"])
            if num==0:
                trimmed_reads_filename=options.selected_sample_all_reads_vector_trimmed[file_num]
                junction_reads_filename=options.selected_sample_fusion_reads[file_num]
                trimming_stats_filename=options.selected_sample_trimming_stats[file_num]
                if options.selected_ended=="PE" and options.background_ended=="PE":
                    if file_num%2==1:
                        trimmed_reads_filename_mate1=options.selected_sample_all_reads_vector_trimmed[file_num-1]
                        junction_reads_filename_mate1=options.selected_sample_fusion_reads[file_num-1]
                        trimmed_reads_filename_mate2=trimmed_reads_filename
                        junction_reads_filename_mate2=junction_reads_filename
                        filename_mate2=filename
                        filename_mate1=eachtype[file_num-1]
            else:
                trimmed_reads_filename=options.background_sample_all_reads_vector_trimmed[file_num]
                junction_reads_filename=options.background_sample_fusion_reads[file_num]
                trimming_stats_filename=options.background_sample_trimming_stats[file_num]
                if options.selected_ended=="PE" and options.background_ended=="PE":
                    if file_num%2==1:
                        trimmed_reads_filename_mate1=options.background_sample_all_reads_vector_trimmed[file_num-1]
                        junction_reads_filename_mate1=options.background_sample_fusion_reads[file_num-1]
                        trimmed_reads_filename_mate2=trimmed_reads_filename
                        junction_reads_filename_mate2=junction_reads_filename
                        filename_mate2=filename
                        filename_mate1=eachtype[file_num-1]
            if options.selected_ended=="PE" and options.background_ended=="PE":
                allinputs.append([filename_mate1,filename_mate2,options,reads_to_be_trimmed,trimmed_reads_filename_mate1,trimmed_reads_filename_mate2,junction_reads_filename_mate1,junction_reads_filename_mate2,trimming_stats_filename,results_dict[filename]["reads_discarded"]])
            else:
                allinputs.append([filename,options,reads_to_be_trimmed,trimmed_reads_filename,junction_reads_filename,trimming_stats_filename,results_dict[filename]["reads_discarded"]])
    """print(len(allinputs),options.selected_ended, options.background_ended)
    sys.stdout.flush()"""   
    if options.selected_ended=="PE" and options.background_ended=="PE":
        """print("Calling function for trimming")
        sys.stdout.flush()"""
        pool.map(trimVectorFromReadsPairedEnded,allinputs)
    else:
        pool.map(trimVectorFromReadsSingleEnded,allinputs)

def reAlignReadsMappedToVector(options):
    """
    Selects fusion reads which are not present in the transcriptome file
    and realign those to the genome and update both genome and
    transcriptome files
    """
    
    # Mapping fusion reads to genome
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            #filename=filename.split("/")[-1].split(".fastq")[0]
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==0:continue
            cmd="STAR "
            cmd+=" --runThreadN "+str(options.CPU)+" "
            cmd+=" --genomeDir "+options.star_genome_index
            cmd+=" --genomeLoad LoadAndKeep "
            if num==0:
                if options.selected_ended=="SE" and options.background_ended=="SE":
                    cmd+=" --readFilesIn "+options.selected_sample_fusion_reads[file_num]
                else:
                    cmd+=" --readFilesIn "+options.selected_sample_fusion_reads[file_num-1]+" "+options.selected_sample_fusion_reads[file_num]
                cmd+=" --outFileNamePrefix "+options.selected_sample_STAR_prefix_round2[file_num]
            else:
                if options.selected_ended=="SE" and options.background_ended=="SE":
                    cmd+=" --readFilesIn "+options.background_sample_fusion_reads[file_num]
                else:
                    cmd+=" --readFilesIn "+options.background_sample_fusion_reads[file_num-1]+" "+options.background_sample_fusion_reads[file_num]
                cmd+=" --outFileNamePrefix "+options.background_sample_STAR_prefix_round2[file_num]
            cmd+=" --outSAMtype SAM "
            #cmd+=" --outReadsUnmapped Fastx "
            cmd+=" --outFilterMultimapNmax 500 "
            
            cmd+=" --outFilterScoreMinOverLread 0.3 --outFilterMatchNminOverLread 0.3 "
            cmd+=" --alignIntronMax 10000 "
            #cmd+=" --quantMode TranscriptomeSAM "
            #cmd+=" --quantTranscriptomeBan Singleend "
            if num==0:
                cmd+=" > "+options.selected_sample_STAR_round2_output[file_num]
                cmd+=" 2> "+options.selected_sample_STAR_round2_error[file_num]
            else:
                cmd+=" > "+options.background_sample_STAR_round2_output[file_num]
                cmd+=" 2> "+options.background_sample_STAR_round2_error[file_num]
            os.system(cmd)
            #print(cmd)
            if num==0:
                cmd="rm "+options.selected_sample_STAR_prefix_round2[file_num]+"Log.out "
                cmd+=options.selected_sample_STAR_prefix_round2[file_num]+"Log.progress.out "
                cmd+=options.selected_sample_STAR_prefix_round2[file_num]+"SJ.out.tab "
                cmd+=options.selected_sample_STAR_round2_output[file_num]
            else:
                cmd="rm "+options.background_sample_STAR_prefix_round2[file_num]+"Log.out "
                cmd+=options.background_sample_STAR_prefix_round2[file_num]+"Log.progress.out "
                cmd+=options.background_sample_STAR_prefix_round2[file_num]+"SJ.out.tab "
                cmd+=options.background_sample_STAR_round2_output[file_num]
            os.system(cmd)
            
            if num==0 and os.stat(options.selected_sample_STAR_round2_error[file_num]).st_size == 0:
                cmd="rm "+options.selected_sample_STAR_round2_error[file_num]
            elif num==1 and os.stat(options.background_sample_STAR_round2_error[file_num]).st_size == 0:
                cmd="rm "+options.background_sample_STAR_round2_error[file_num]
            os.system(cmd)
            
            #logger.info("STAR round2 mapping for "+filename+" completed")
            
            """if num==0:
                cmd="cp "+options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num]+" "+options.selected_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num]
                os.system(cmd)
            else:
                cmd="cp "+options.background_sample_STAR_transcriptome_bamfilename_round2[file_num]+" "+options.background_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num]
                os.system(cmd)"""
            
    cmd="STAR "
    cmd+=" --genomeLoad Remove "
    cmd+=" --genomeDir "+options.star_genome_index
    os.system(cmd)
    
    
    # Mapping fusion reads to transcriptome - Need to repeat this step since STAR is buggy 
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            #filename=filename.split("/")[-1].split(".fastq")[0]
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==0:continue
            cmd="STAR "
            cmd+=" --runThreadN "+str(options.CPU)+" "
            #cmd+=" --genomeDir "+options.star_genome_index
            cmd+=" --genomeDir "+options.transcriptome_index
            cmd+=" --genomeLoad LoadAndKeep "
            if num==0:
                if options.selected_ended=="SE" and options.background_ended=="SE":
                    cmd+=" --readFilesIn "+options.selected_sample_fusion_reads[file_num]
                else:
                    cmd+=" --readFilesIn "+options.selected_sample_fusion_reads[file_num-1]+" "+options.selected_sample_fusion_reads[file_num]
                cmd+=" --outFileNamePrefix "+options.selected_sample_STAR_prefix_round2[file_num]+"_transcriptome_"
            else:
                if options.selected_ended=="SE" and options.background_ended=="SE":
                    cmd+=" --readFilesIn "+options.background_sample_fusion_reads[file_num]
                else:
                    cmd+=" --readFilesIn "+options.background_sample_fusion_reads[file_num-1]+" "+options.background_sample_fusion_reads[file_num]
                cmd+=" --outFileNamePrefix "+options.background_sample_STAR_prefix_round2[file_num]+"_transcriptome_"
            cmd+=" --outSAMtype SAM "
            #cmd+=" --outReadsUnmapped Fastx "
            cmd+=" --outFilterMultimapNmax 500 "
            cmd+=" --seedPerReadNmax 5000 "
            cmd+=" --seedPerWindowNmax 100 "
            cmd+=" --outFilterScoreMinOverLread 0.8 --outFilterMatchNminOverLread 0.85 "
            cmd+=" --alignIntronMax 10000 "
            #cmd+=" --quantMode TranscriptomeSAM "
            #cmd+=" --quantTranscriptomeBan Singleend "
            if num==0:
                cmd+=" > "+options.selected_sample_STAR_round2_output[file_num]
                cmd+=" 2> "+options.selected_sample_STAR_round2_error[file_num]
            else:
                cmd+=" > "+options.background_sample_STAR_round2_output[file_num]
                cmd+=" 2> "+options.background_sample_STAR_round2_error[file_num]
            #print(cmd)
            os.system(cmd)
            #print(cmd)
            #continue
            cmd="samtools view -bSh "
            if num==0:
                cmd+=" "+options.selected_sample_STAR_prefix_round2[file_num]+"_transcriptome_Aligned.out.sam "
                cmd+=" > "+options.selected_sample_STAR_prefix_round2[file_num]+"Aligned.toTranscriptome.out.bam"
            else:
                cmd+=" "+options.background_sample_STAR_prefix_round2[file_num]+"_transcriptome_Aligned.out.sam "
                cmd+=" > "+options.background_sample_STAR_prefix_round2[file_num]+"Aligned.toTranscriptome.out.bam"
            os.system(cmd)
            #print(cmd)
            
            if num==0:
                cmd="cp "+options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num]+" "
                cmd+=options.selected_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num]
                #print(cmd)
                os.system(cmd)
            else:
                cmd="cp "+options.background_sample_STAR_transcriptome_bamfilename_round2[file_num]+" "
                cmd+=options.background_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num]
                #print(cmd)
                os.system(cmd)
            
            
            #logger.info("STAR round2 mapping for "+filename+" completed")
    cmd="STAR "
    cmd+=" --genomeLoad Remove "
    cmd+=" --genomeDir "+options.transcriptome_index
    os.system(cmd)
 
def prepareEachGenomeFileSingleEnded(input):
    """
    """
    genome_alignment_file,options,all_reads_filename,output_genome_alignment_file,genome_alignment_file_round1,logger_proxy,logging_mutex=input
    #print("Input filename",genome_alignment_file)
    #print("Output filename",output_genome_alignment_file)
    #sys.stdout.flush()
    # Add the vector sequences to the trimmed reads
    all_reads=readFastqFile(all_reads_filename)
    """print(all_reads_filename)
    sys.stdout.flush() """ 
    #output_genome_alignment_file=genome_alignment_file[:-3]+"_modified.sam"
    fhr=open(genome_alignment_file,"r")
    fhw=open(output_genome_alignment_file,"w")
    reads_processed=[]
    for line in fhr:
        if line[0]=="@":
            fhw.write(line)
            continue
        line=line.strip().split()
        reads_processed.append(line[0].split("_")[0])
        if "5_prime_end_forward" in line[0]:
            read_id=line[0].split("_")[0]
            start,end=int(line[0].split("_")[-2]),int(line[0].split("_")[-1])
            if start!=1:
                vector_start,vector_end=1,start-1
            else:
                vector_start,vector_end=end+1,len(all_reads[read_id][0])
            vector_seq=all_reads[read_id][0][:vector_end]
            cigar=line[5]
            cigar_values=[int(s) for s in re.findall(r'\d+',cigar)]
            cigar_alphabets=re.findall(r'[A-Z]',cigar)
            #if int(line[1]) in [0,256]:
            if isKthBitSet(int(line[1]), 5)==False:   
                if cigar_alphabets[0]=="S":
                    cigar_values[0]=int(cigar_values[0])+vector_end-vector_start+1
                else:
                    cigar_values.insert(0,str(vector_end-vector_start+1))
                    cigar_alphabets.insert(0,"S")
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                line[9]=all_reads[read_id][0]
                line[10]=all_reads[read_id][2]
                line.append("FR:i:1")
                fhw.write("\t".join(line)+"\n")
                #print(cigar,new_cigar)
                sys.stdout.flush()
            else:
                if cigar_alphabets[-1]=="S":
                    cigar_values[-1]=int(cigar_values[-1])+vector_end-vector_start+1
                else:
                    cigar_values.append(str(vector_end-vector_start+1))
                    cigar_alphabets.append("S")
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                line[9]=reverseComplement(all_reads[read_id][0])
                line[10]=all_reads[read_id][2][::-1]
                line.append("FR:i:1")
                fhw.write("\t".join(line)+"\n")
                #print(cigar,new_cigar)
                sys.stdout.flush()
        elif "5_prime_end_reverse" in line[0]:
            read_id=line[0].split("_")[0]
            start,end=int(line[0].split("_")[-2]),int(line[0].split("_")[-1])
            if start==1:
                vector_start,vector_end=end+1,len(all_reads[read_id][0])
            else:
                vector_start,vector_end=1,start-1
            #vector_seq=all_reads[read_id][0][vector_start-1:]
            cigar=line[5]
            cigar_values=[int(s) for s in re.findall(r'\d+',cigar)]
            cigar_alphabets=re.findall(r'[A-Z]',cigar)
            if isKthBitSet(int(line[1]), 5)==False:  
                if cigar_alphabets[-1]=="S":
                    cigar_values[-1]=int(cigar_values[-1])+vector_end-vector_start+1
                else:
                    cigar_values.append(str(vector_end-vector_start+1))
                    cigar_alphabets.append("S")
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                line[9]=all_reads[read_id][0]
                line[10]=all_reads[read_id][2]
                line.append("FR:i:1")
                fhw.write("\t".join(line)+"\n")
                #print(cigar,new_cigar)
                sys.stdout.flush()
            else:
                if cigar_alphabets[0]=="S":
                    cigar_values[0]=int(cigar_values[0])+vector_end-vector_start+1
                else:
                    cigar_values.insert(0,str(vector_end-vector_start+1))
                    cigar_alphabets.insert(0,"S")
                #temp=[iter(cigar_values),iter(cigar_alphabets)]
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                line[9]=reverseComplement(all_reads[read_id][0])
                line[10]=all_reads[read_id][2][::-1]
                line.append("FR:i:1")
                fhw.write("\t".join(line)+"\n")
                """if line[0]=="J00102:13:HTW3WBBXX:1:2205:7253:47436_5_prime_end_reverse_84_149":
                    print(vector_start,vector_end)
                    #print("vector ",vector_seq)
                    print(cigar_values)
                    print(cigar_alphabets)
                    print(new_cigar)
                    print(line)"""
        elif "3_prime_end" in line[0]:
            line.append("FR:i:1")
            fhw.write("\t".join(line)+"\n")
        else:
            line.append("FR:i:0")
            fhw.write("\t".join(line)+"\n")
    fhr.close()
    
    reads_processed=set(reads_processed)
    fhr=open(genome_alignment_file_round1,"r")
    for line in fhr:
        if line[0]=="@":
            continue
        if line.split()[0] not in reads_processed:
            fhw.write(line.strip()+"\tFR:i:0"+"\n")
    fhr.close()
    fhw.close()
    
    # Sort and create indices 
    cmd="samtools view -Sb "+output_genome_alignment_file+"|samtools sort - > "+output_genome_alignment_file+"sorted"
    os.system(cmd)
    cmd="mv "+output_genome_alignment_file+"sorted "+output_genome_alignment_file
    os.system(cmd)
    cmd="samtools index "+output_genome_alignment_file
    os.system(cmd)
    
    with logging_mutex:
        logger_proxy.info("Preparing file for genome browser viewing over for "+output_genome_alignment_file)
    #logger.info("Preparing file for genome browser viewing over for "+output_genome_alignment_file)

def prepareEachGenomeFilePairedEnded(input):
    """
    """
    pprint.pprint(input)
    genome_alignment_file_round2,options,all_reads_filename_mate1,all_reads_filename_mate2,output_genome_alignment_file,genome_alignment_file_round1,fusion_reads_filename_mate1,fusion_reads_filename_mate2,logger_proxy,logging_mutex=input
    # Add the vector sequences to the trimmed reads
    all_reads_mate1=readFastqFile(all_reads_filename_mate1)
    all_reads_mate2=readFastqFile(all_reads_filename_mate2)
    fusion_reads_mate1=readFastqFile(fusion_reads_filename_mate1)
    fusion_reads_mate2=readFastqFile(fusion_reads_filename_mate2)
    #output_genome_alignment_file=genome_alignment_file[:-3]+"_modified.sam"
    fhr=open(genome_alignment_file_round2,"r")
    fhw=open(output_genome_alignment_file,"w")
    reads_processed=[]
    for line in fhr:
        if line[0]=="@":
            fhw.write(line)
            continue
        line=line.strip().split()
        if isKthBitSet(int(line[1]),7)==True:
            if line[0] not in fusion_reads_mate1:
                """print("skipping")
                sys.stdout.flush()"""
                line.append("FR:i:0")
                fhw.write("\t".join(line)+"\n")
                continue
        else:
            if line[0] not in fusion_reads_mate2:
                """print("skipping")
                sys.stdout.flush()"""
                line.append("FR:i:0")
                fhw.write("\t".join(line)+"\n")
                continue
        reads_processed.append(line[0].split("_")[0]+line[1])
        if "5_prime_end_forward" in line[0] and "S" in line[5]:
            read_id=line[0].split("_")[0]
            start,end=int(line[0].split("_")[-2]),int(line[0].split("_")[-1])
            cigar=line[5]
            cigar_values=[int(s) for s in re.findall(r'\d+',cigar)]
            cigar_alphabets=re.findall(r'[A-Z]',cigar)
            """if isKthBitSet(int(line[1]), 7)==True:
                vector_seq=all_reads_mate1[read_id][0][:vector_end]
            else:
                vector_seq=all_reads_mate2[read_id][0][:vector_end]"""
            #if int(line[1]) in [0,256]:
            if isKthBitSet(int(line[1]), 5)==False: 
                if start!=1:
                    vector_start,vector_end=1,start-1
                else:
                    vector_start,vector_end=end+1,len(all_reads_filename_mate1[read_id][0])
                #Forward reads  
                if cigar_alphabets[0]=="S":
                    cigar_values[0]=int(cigar_values[0])+vector_end-vector_start+1
                else:
                    cigar_values.insert(0,str(vector_end-vector_start+1))
                    cigar_alphabets.insert(0,"S")
                old_line=line[:]
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                if isKthBitSet(int(line[1]), 7)==True:
                    line[9]=all_reads_mate1[read_id][0]
                    line[10]=all_reads_mate1[read_id][2]
                else:
                    line[9]=all_reads_mate2[read_id][0]
                    line[10]=all_reads_mate2[read_id][2]
                line.append("FR:i:1")
                cigar_values=[int(s) for s in re.findall(r'\d+',new_cigar)]
                cigar_alphabets=re.findall(r'[A-Z]',new_cigar)
                if sum([cigar_values[i] for i in range(len(cigar_alphabets)) if cigar_alphabets[i]!="N"])!=len(line[9]):
                    #print("CIGAR",line[0],line[1],cigar,new_cigar)
                    fhw.write("\t".join(old_line)+"\n")
                    sys.stdout.flush()
                else:
                    fhw.write("\t".join(line)+"\n")
                    """print("Writing new line")
                    sys.stdout.flush()"""
            else:
                # Reverse reads
                if start!=1:
                    vector_start,vector_end=1,start-1
                else:
                    vector_start,vector_end=end+1,len(all_reads_filename_mate2[read_id][0])
                if cigar_alphabets[-1]=="S":
                    cigar_values[-1]=int(cigar_values[-1])+vector_end-vector_start+1
                else:
                    cigar_values.append(str(vector_end-vector_start+1))
                    cigar_alphabets.append("S")
                    
                
                old_line=line[:]
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                if isKthBitSet(int(line[1]), 7)==True:
                    line[9]=reverseComplement(all_reads_mate1[read_id][0])
                    line[10]=all_reads_mate1[read_id][2][::-1]
                else:
                    line[9]=reverseComplement(all_reads_mate2[read_id][0])
                    line[10]=all_reads_mate2[read_id][2][::-1]
                
                line.append("FR:i:1")
                fhw.write("\t".join(line)+"\n")
                cigar_values=[int(s) for s in re.findall(r'\d+',new_cigar)]
                cigar_alphabets=re.findall(r'[A-Z]',new_cigar)
                if sum([cigar_values[i] for i in range(len(cigar_alphabets)) if cigar_alphabets[i]!="N"])!=len(line[9]):
                    #print("CIGAR",line[0],line[1],cigar,new_cigar)
                    fhw.write("\t".join(old_line)+"\n")
                    sys.stdout.flush()
                else:
                    fhw.write("\t".join(line)+"\n")
                    """print("Writing new line")
                    sys.stdout.flush()"""
        elif "5_prime_end_reverse" in line[0]  and "S" in line[5]:
            read_id=line[0].split("_")[0]
            start,end=int(line[0].split("_")[-2]),int(line[0].split("_")[-1])
            cigar=line[5]
            cigar_values=[int(s) for s in re.findall(r'\d+',cigar)]
            cigar_alphabets=re.findall(r'[A-Z]',cigar)
            #if int(line[1]) in [0,256]:   
            if isKthBitSet(int(line[1]), 5)==False:  
                if isKthBitSet(int(line[1]), 7)==True:
                    if start==1:
                        vector_start,vector_end=end+1,len(all_reads_mate1[read_id][0])
                    else:
                        vector_start,vector_end=1,start-1
                    #vector_start,vector_end=end+1,len(all_reads_mate1[read_id][0])
                    #vector_seq=all_reads_mate1[read_id][0][vector_start-1:]
                else:
                    if start==1:
                        vector_start,vector_end=end+1,len(all_reads_mate2[read_id][0])
                    else:
                        vector_start,vector_end=1,start-1
                    #vector_start,vector_end=end+1,len(all_reads_mate2[read_id][0])
                    #vector_seq=all_reads_mate2[read_id][0][vector_start-1:]
                
                
                if cigar_alphabets[-1]=="S":
                    cigar_values[-1]=int(cigar_values[-1])+vector_end-vector_start+1
                else:
                    cigar_values.append(str(vector_end-vector_start+1))
                    cigar_alphabets.append("S")
                #temp=[iter(cigar_values),iter(cigar_alphabets)]
                old_line=line[:]
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                if isKthBitSet(int(line[1]), 7)==True:
                    line[9]=all_reads_mate1[read_id][0]
                    line[10]=all_reads_mate1[read_id][2]
                else:
                    line[9]=all_reads_mate2[read_id][0]
                    line[10]=all_reads_mate2[read_id][2]
                
                line.append("FR:i:1")
                fhw.write("\t".join(line)+"\n")
                cigar_values=[int(s) for s in re.findall(r'\d+',new_cigar)]
                cigar_alphabets=re.findall(r'[A-Z]',new_cigar)
                if sum([cigar_values[i] for i in range(len(cigar_alphabets)) if cigar_alphabets[i]!="N"])!=len(line[9]):
                    #print("CIGAR",line[0],line[1],cigar,new_cigar)
                    fhw.write("\t".join(old_line)+"\n")
                    sys.stdout.flush()
                else:
                    fhw.write("\t".join(line)+"\n")
                    """print("Writing new line")
                    sys.stdout.flush()"""
            else:
                if isKthBitSet(int(line[1]), 7)==True:
                    if start==1:
                        vector_start,vector_end=end+1,len(all_reads_mate1[read_id][0])
                    else:
                        vector_start,vector_end=1,start-1
                else:
                    if start==1:
                        vector_start,vector_end=end+1,len(all_reads_mate2[read_id][0])
                    else:
                        vector_start,vector_end=1,start-1
                        
                if cigar_alphabets[0]=="S":
                    cigar_values[0]=int(cigar_values[0])+vector_end-vector_start+1
                else:
                    cigar_values.insert(0,str(vector_end-vector_start+1))
                    cigar_alphabets.insert(0,"S")
                #temp=[iter(cigar_values),iter(cigar_alphabets)]
                old_line=line[:]
                cigar_values=list(map(str,cigar_values))
                new_cigar="".join(list(itertools.chain(*zip(cigar_values, cigar_alphabets))))
                line[5]=new_cigar
                if isKthBitSet(int(line[1]), 7)==True:
                    line[9]=reverseComplement(all_reads_mate1[read_id][0])
                    line[10]=all_reads_mate1[read_id][2][::-1]
                else:
                    line[9]=reverseComplement(all_reads_mate2[read_id][0])
                    line[10]=all_reads_mate2[read_id][2][::-1]
                line.append("FR:i:1")
                fhw.write("\t".join(line)+"\n")
                cigar_values=[int(s) for s in re.findall(r'\d+',new_cigar)]
                cigar_alphabets=re.findall(r'[A-Z]',new_cigar)
                if sum([cigar_values[i] for i in range(len(cigar_alphabets)) if cigar_alphabets[i]!="N"])!=len(line[9]):
                    #print("CIGAR",line[0],line[1],cigar,new_cigar)
                    fhw.write("\t".join(old_line)+"\n")
                    sys.stdout.flush()
                else:
                    fhw.write("\t".join(line)+"\n")
                    """print("Writing new line")
                    sys.stdout.flush()"""
        else:
            line.append("FR:i:0")
            fhw.write("\t".join(line)+"\n")
    fhr.close()
    
    reads_processed=set(reads_processed)
    fhr=open(genome_alignment_file_round1,"r")
    for line in fhr:
        if line[0]=="@":
            continue
        if line.split()[0]+line.split()[1] not in reads_processed:
            fhw.write(line.strip()+"\tFR:i:0"+"\n")
    fhr.close()
    fhw.close()
    
    # Sort and create indices 
    cmd="samtools view -Sb "+output_genome_alignment_file+"|samtools sort - > "+output_genome_alignment_file+"sorted"
    os.system(cmd)
    cmd="mv "+output_genome_alignment_file+"sorted "+output_genome_alignment_file
    os.system(cmd)
    cmd="samtools index "+output_genome_alignment_file
    os.system(cmd)
    
    with logging_mutex:
        logger_proxy.info("Preparing file for genome browser viewing over for "+output_genome_alignment_file)
    #logger.info("Preparing file for genome browser viewing over for "+output_genome_alignment_file)

def prepareGenomeFilesForGenomeBrowser(options,logger_proxy,logging_mutex):
    """
    Append reads from round1 which are absent in round2
    """
    pool = multiprocessing.Pool(processes=int(options.CPU))
    allinputs=[]
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if num==0:
                if options.selected_ended=="SE" and options.background_ended=="SE":
                    pass
                else:
                    if file_num%2==1:
                        pass
                    else:
                        continue
                genome_alignment_file=options.selected_sample_STAR_genome_filename_round2[file_num]
                all_reads_filename=options.selected_sample_N_removed[file_num]
                output_genome_alignment_file=options.selected_sample_genome_browser_per_replicate[file_num]
                genome_alignment_file_round1=options.selected_sample_STAR_genome_filename_round1[file_num]
                #print(genome_alignment_file,all_reads_filename,output_genome_alignment_file,genome_alignment_file_round1)
                #allinputs.append([genome_alignment_file,options,all_reads_filename,output_genome_alignment_file,genome_alignment_file_round1])
                if options.selected_ended=="PE" and options.background_ended=="PE":
                    fusion_reads_filename_mate1=options.selected_sample_fusion_reads[file_num-1]
                    fusion_reads_filename_mate2=options.selected_sample_fusion_reads[file_num]
                    allinputs.append([genome_alignment_file,options,options.selected_sample_N_removed[file_num-1],all_reads_filename,output_genome_alignment_file,genome_alignment_file_round1,fusion_reads_filename_mate1,fusion_reads_filename_mate2,logger_proxy,logging_mutex])
                else:
                    allinputs.append([genome_alignment_file,options,all_reads_filename,output_genome_alignment_file,genome_alignment_file_round1,logger_proxy,logging_mutex])
    if options.selected_ended=="PE" and options.background_ended=="PE":
        pool.map(prepareEachGenomeFilePairedEnded,allinputs)
    else:
        pool.map(prepareEachGenomeFileSingleEnded,allinputs)
    
    # Merge all the replicates together
    if options.selected_ended=="SE" and options.background_ended=="SE":
        cmd="samtools merge -f "+options.selected_sample_genome_browser
        for filename in options.selected_sample_genome_browser_per_replicate:
            cmd+=" "+filename
        os.system(cmd)
        cmd="samtools index "+options.selected_sample_genome_browser
        os.system(cmd)
    else:
        cmd="samtools merge -f "+options.selected_sample_genome_browser
        for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
            for file_num,filename in enumerate(eachtype):
                if num==0:
                    if file_num%2==1:
                        pass
                    else:
                        continue
                    cmd+=" "+options.selected_sample_genome_browser_per_replicate[file_num]
        os.system(cmd)

def prepareTranscriptomeFiles(options,logger_proxy,logging_mutex):
    """
    """
    # Merge the transcriptome files from round1 and round2
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if num==0:
                if options.selected_ended=="PE" and options.background_ended=="PE":
                    if file_num%2==1:
                        pass
                    else:
                        continue
                cmd="samtools cat -o "+options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num]+"tempfile "
                cmd+=options.selected_sample_STAR_transcriptome_bamfilename_round1[file_num]+" "
                cmd+=options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num]
                os.system(cmd)
                cmd="mv "+options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num]+"tempfile "
                cmd+=options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num]
                os.system(cmd)
                with logging_mutex:
                    logger_proxy.info("Preparing transcriptome files completed for "+options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num])
                #logger.info("Preparing transcriptome files completed for "+options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num])
            else:
                if options.selected_ended=="PE" and options.background_ended=="PE":
                    if file_num%2==1:
                        pass
                    else:
                        continue
                cmd="samtools cat -o "+options.background_sample_STAR_transcriptome_bamfilename_round2[file_num]+"tempfile "
                cmd+=options.background_sample_STAR_transcriptome_bamfilename_round1[file_num]+" "
                cmd+=options.background_sample_STAR_transcriptome_bamfilename_round2[file_num]
                os.system(cmd)
                cmd="mv "+options.background_sample_STAR_transcriptome_bamfilename_round2[file_num]+"tempfile "
                cmd+=options.background_sample_STAR_transcriptome_bamfilename_round2[file_num]
                os.system(cmd)
                with logging_mutex:
                    logger_proxy.info("Preparing transcriptome files completed for "+options.background_sample_STAR_transcriptome_bamfilename_round2[file_num])
                #logger.info("Preparing transcriptome files completed for "+options.background_sample_STAR_transcriptome_bamfilename_round2[file_num])

def runSalmonToGenerateCounts(options,logger_proxy,logging_mutex):
    """
    Use Salmon to generate counts. Then perform differential analysis.
    """
    # Combine Salmon fragments to one
    """cmd="cat lib/salmon-latest_linux_x86_64/bin/salmonaa lib/salmon-latest_linux_x86_64/bin/salmonab lib/salmon-latest_linux_x86_64/bin/salmonac > "+options.output_directory+"/salmon"
    os.system(cmd)
    cmd="chmod 777 "+options.output_directory+"/salmon"
    os.system(cmd)"""
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            if num==0:  
                inputfilename=options.selected_sample_STAR_transcriptome_bamfilename_round2[file_num]
                outputfilename=options.selected_sample_salmon_counts_outputfile[file_num]
                errorfilename=options.selected_sample_salmon_counts_error[file_num]
            else:
                inputfilename=options.background_sample_STAR_transcriptome_bamfilename_round2[file_num]
                outputfilename=options.background_sample_salmon_counts_outputfile[file_num]
                errorfilename=options.background_sample_salmon_counts_error[file_num]
            cmd="salmon quant "
            #cmd=options.output_directory+"/salmon quant "
            cmd+=" -t "+options.transcriptome
            cmd+=" --libType A "
            cmd+=" -a "+inputfilename
            cmd+=" -p "+str(options.CPU)+" "
            cmd+=" -o "+outputfilename
            cmd+=" -g "+options.transcript_to_gene_map +" "
            cmd+=" -s "
            cmd+=" 2> "+errorfilename
            os.system(cmd)
            with logging_mutex:
                logger_proxy.info("Salmon run completed for "+inputfilename)
            #logger.info("Salmon run completed for "+inputfilename)
            """cmd="lib/samtools/samtools sort -@ "+str(options.CPU)+" "+inputfilename+" > "+inputfilename+"sorted"
            os.system(cmd)
            cmd="mv "+inputfilename+"sorted "+inputfilename
            os.system(cmd)
            cmd="lib/samtools/samtools index "+inputfilename
            os.system(cmd)"""
    
    cmd="paste <(cut -f1 "+outputfilename+"/quant.genes.sf|tail -n +2) "
    for num,eachtype in enumerate([options.selected_sample_N_removed,options.background_sample_N_removed]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            if num==0:  
                outputfilename=options.selected_sample_salmon_counts_outputfile[file_num]
            else:
                outputfilename=options.background_sample_salmon_counts_outputfile[file_num]
            cmd+=" <(cut -f4 "+outputfilename+"/quant.genes.sf|tail -n +2) "
    cmd+=" > "+options.salmon_gene_counts_matrix
    subprocess.check_call(['bash', '-c', cmd])

def performEnrichmentAnalysis(options):
    """
    Calls the R program to output DGE list of genes
    """
    if options.selected_ended=="SE" and options.background_ended=="SE":
        cmd="perform_DE_analysis_deseq2.R "
        cmd+=options.output_directory+" "
        cmd+=options.salmon_gene_counts_matrix.split("/")[-1]
        cmd+=" "+str(0.05)+" "
        cmd+=" ".join(options.selected_sample)+" "
        cmd+=" ".join(options.background_sample)+" "
        cmd+=" > "+options.output_directory+"/deseq2.output "
        cmd+=" 2> "+options.output_directory+"/deseq2.error "
        os.system(cmd)
    else:
        cmd="perform_DE_analysis_deseq2.R "
        cmd+=options.output_directory+" "
        cmd+=options.salmon_gene_counts_matrix.split("/")[-1]
        cmd+=" "+str(0.05)+" "
        cmd+=" ".join(options.selected_sample[0:len(options.selected_sample):2])+" "
        cmd+=" ".join(options.background_sample[0:len(options.background_sample):2])+" "
        cmd+=" > "+options.output_directory+"/deseq2.output "
        cmd+=" 2> "+options.output_directory+"/deseq2.error "
        os.system(cmd)
    print(cmd)

def createJunctionFileForBackgroundLibraries(inputs):
    inputfilename,outputfilename=inputs
    samoutputfilename=outputfilename[:-3]+"sam"
    cmd="samtools view -H "+inputfilename+" > "+samoutputfilename
    os.system(cmd)
    cmd="samtools view "+inputfilename+"|grep prime >> "+samoutputfilename
    os.system(cmd)
    cmd="samtools view -Sb -o "+outputfilename+" "+samoutputfilename
    os.system(cmd)

def computeInFrameInfoForEachJunctionRead(inputs):
    """
    Performs the following operations:
        - Takes as input a bamfile for 5' junction reads
        - Computes whether the expression was in-frame
        - If expression was in-frame, then it appends the alignment with portion of the read which is trimmed off
    """
    bamfilename,options,gene_info=inputs
    
    samfilename=bamfilename[:-3]+"sam"
    cmd="samtools view -h "+bamfilename+" > "+samfilename
    os.system(cmd)
    
    number_of_cigars=[[] for i in range(1000)] # ith position stores the number of reads which have i nucleotides soft clipped from the 5' end
    samfilename_junction_info_appended=samfilename[:-4]+"_junction_info_appended.sam"
    fhr=open(samfilename,"r")
    fhw=open(samfilename_junction_info_appended,"w")
    
    for line in fhr:
        bases_soft_clipped=-1
        if line[0]=="@":
            fhw.write(line)
            continue
        #print(len(line.strip().split("\t")),line.strip().split("\t"))
        read_id,orientation,transcript_id,starting_loc,useless1,cigar,useless2,useless3,useless4,read_seq,read_qual=line.strip().split("\t")[:11]
        if transcript_id not in gene_info:continue
        if re.search(r'^\d*M$',cigar) or re.search(r'^\d*M\d*S$',cigar): # No Soft clipping at 5'end
            number_of_cigars[0].append(read_id)
            portion_of_read_unmapped=""
            portion_of_read_mapped=read_seq
        elif re.search(r'^\d*S',cigar):# Some Soft clipping at 5'end
            bases_soft_clipped=[int(s) for s in re.findall(r'-?\d+\.?\d*', cigar)][0]
            number_of_cigars[bases_soft_clipped].append(read_id)
            portion_of_read_unmapped=read_seq[:bases_soft_clipped]
            portion_of_read_mapped=read_seq[bases_soft_clipped:]
        starting_loc=int(starting_loc)
        if bases_soft_clipped==-1 or starting_loc>=gene_info[transcript_id]["cds_end"]:
            line=line.strip()+"\t"+"FN:0"
            fhw.write(line+"\n")
            continue
        if bases_soft_clipped % 3 == 0:
            if (starting_loc - gene_info[transcript_id]["cds_start"]) % 3 == 0:
                line=line.strip()+"\t"+"FI:"+portion_of_read_unmapped+":"+str(starting_loc)
            else:
                line=line.strip()+"\t"+"FN:0"
        elif bases_soft_clipped % 3 == 1:
            if (starting_loc + 2 - gene_info[transcript_id]["cds_start"]) % 3 == 0:
                line=line.strip()+"\t"+"FI:"+portion_of_read_unmapped+":"+str(starting_loc)
            else:
                line=line.strip()+"\t"+"FN:0"
        elif bases_soft_clipped % 3 == 2:
            if (starting_loc + 1 - gene_info[transcript_id]["cds_start"]) % 3 == 0:
                line=line.strip()+"\t"+"FI:"+portion_of_read_unmapped+":"+str(starting_loc)
            else:
                line=line.strip()+"\t"+"FN:0"
        fhw.write(line+"\n")
    fhw.close() 
    fhr.close()

def collectInformationAboutEnrichmentAnalysis(salmon_DGE_filename,salmon_normalized_counts,gene_info):
    """
    """
    info={}
    fhr=open(salmon_normalized_counts,"r")
    for line_num,line in enumerate(fhr):
        if line_num==0:
            line=line.strip().split(",")[1:]
            header=[l.strip("\"") for l in line]
        else:
            gene=line.strip().split(",")[0].strip("\"")
            info[gene]={"norm_counts":{header[i]:val for i,val in enumerate(line.strip().split(",")[1:])}}
            info[gene]["log2FoldChange"]=-1
            info[gene]["padj"]=-1
    fhr.close()
    
    fhr=open(salmon_DGE_filename,"r")
    for line_num,line in enumerate(fhr):
        if line_num==0:continue
        gene=line.strip().split(",")[0].strip("\"")
        info[gene]["log2FoldChange"]=line.strip().split(",")[2]
        info[gene]["padj"]=line.strip().split(",")[-1]
    fhr.close()
    
    genes=list(set([gene_info[transcript]["gene"] for transcript in gene_info]))
    for gene in genes:
        if gene not in info:
            info[gene]={"log2FoldChange":-1,"padj":-1,"norm_counts":{header[i]:-1 for i in range(len(header))}}
    return info

def compileInformationForEachSample(fusion_reads_samfilename,ended,cpu):
    info={}
    
    # Convert samfile to name sorted
    #print(fusion_reads_samfilename)
    #return
    if ended=="PE":
        """cmd=SAMTOOLS+" view -@ "+str(cpu)+" -Sb "
        cmd+=fusion_reads_samfilename+" | "
        cmd+=SAMTOOLS+" sort -@ "+str(cpu)+" -n | "
        cmd+=SAMTOOLS+" view -@ "+str(cpu)+" > "+fusion_reads_samfilename+".namesorted.sam"
        print(cmd)
        return"""
        #os.system(cmd)
        
        fhr=open(fusion_reads_samfilename,"r")
        reads_out_of_frame=[]
        line_num=0
        for line in fhr:
            line_num+=1
            if line[0]=="@":continue
            if "3_prime_end" in line:
                line_num+=1
                continue
            if isKthBitSet(int(line.strip().split()[1]),4)==True:
                #line_num+=1
                continue
            read_id=line.strip().split()[0]
            transcript_id=line.strip().split()[2]
            next_line=fhr.readline().strip()
            next_read=next_line.split()[0]
            next_transcript_id=next_line.split()[2]
            if read_id!=next_read:
                print(line_num,read_id,next_read)
                print("PROBLEM")
            if transcript_id not in info:
                info[transcript_id]={"num_reads_out_of_frame":0,"reads_in_frame":[],"num_reads_in_frame":0}
            if next_transcript_id not in info:
                info[next_transcript_id]={"num_reads_out_of_frame":0,"reads_in_frame":[],"num_reads_in_frame":0}
                
            if "FN:0" in line and "FN:0" in next_line:
                info[transcript_id]["num_reads_out_of_frame"]+=1
            elif ("FI:" in line or "FI:" not in next_line) or ("FI:" not in line or "FI:" in next_line):
                if "FI" in line:
                    info[transcript_id]["reads_in_frame"].append(line.strip().split("FI:")[-1].strip())
                else:
                    info[next_transcript_id]["reads_in_frame"].append(next_line.strip().split("FI:")[-1].strip())
        fhr.close()
        for transcript_id in info:
            info[transcript_id]["num_reads_in_frame"]=len(info[transcript_id]["reads_in_frame"])
            info[transcript_id]["reads_in_frame"]=list(set(info[transcript_id]["reads_in_frame"]))
        
    else:
        fhr=open(fusion_reads_samfilename,"r")
        for line in fhr:
            if line[0]=="@" or "3_prime_end" in line:continue
            read_id=line.strip().split()[0]
            transcript_id=line.strip().split()[2]
            if transcript_id not in info:
                info[transcript_id]={"num_reads_out_of_frame":0,"reads_in_frame":[],"num_reads_in_frame":0}
            if "FN:0" in line:
                info[transcript_id]["num_reads_out_of_frame"]+=1
            else:
                info[transcript_id]["reads_in_frame"].append(line.strip().split("FI:")[-1].strip())
        fhr.close()
        for transcript_id in info:
            info[transcript_id]["num_reads_in_frame"]=len(info[transcript_id]["reads_in_frame"])
            info[transcript_id]["reads_in_frame"]=list(set(info[transcript_id]["reads_in_frame"]))
    
    """for transcript_id in info:
        print(fusion_reads_samfilename.split("/")[-1],transcript_id,info[transcript_id]["num_reads_in_frame"],info[transcript_id]["num_reads_out_of_frame"])"""
    return info
    
def generateReportFile(options,gene_info,transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region):
    """
    Generates the complete information about all the transcripts and all replicates
    """
    # Check for in-frame fusion reads
    allinputs=[]
    pool = multiprocessing.Pool(processes=int(options.CPU))
    for num,eachtype in enumerate([options.selected_sample,options.background_sample]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            if num==0:
                junction_reads_filename_bam=options.selected_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num]
            else:
                junction_reads_filename_bam=options.background_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num]
            allinputs.append([junction_reads_filename_bam,options,gene_info])
    pool.map(computeInFrameInfoForEachJunctionRead,allinputs)
    
    # Collect gene-level information about enrichment analysis
    enrichment_info=collectInformationAboutEnrichmentAnalysis(options.salmon_DGE_filename,
                                            options.salmon_normalized_counts,
                                            gene_info)
    
    # Collect all relevant information about each transcript and each replicate
    all_info={}
    for num,eachtype in enumerate([options.selected_sample,options.background_sample]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            if num==0:
                fusion_reads_samfilename=options.selected_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num][:-4]+"_junction_info_appended.sam"
            else:
                fusion_reads_samfilename=options.background_sample_STAR_transcriptome_bamfilename_round2_fusion_reads[file_num][:-4]+"_junction_info_appended.sam"
            all_info[filename]=compileInformationForEachSample(fusion_reads_samfilename,options.selected_ended,options.CPU)
            for transcript_id in transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region:
                if transcript_id not in all_info[filename]:
                    all_info[filename][transcript_id]={"num_reads_out_of_frame":-1,"num_reads_in_frame":-1,"reads_in_frame":[]}
    headers=["Transcript_id",
             "Replicate",
             "CDS_start",
             "CDS_end",
             "Length",
             "num_fusion_reads_in_frame_selected_sample",
             "num_fusion_reads_selected_sample",
             "num_fusion_reads_in_frame_background_sample",
             "num_fusion_reads_background_sample",
             "starting_location_of_in_frame_junction_reads",
             "log2FoldChange",
             "padj",
             "norm_counts_selected_sample",
             "norm_counts_background_sample",
             "location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS"
             ]
    transcriptome=readFastaFile(options.transcriptome)
    
    """# Calculate in-frame scores
    for transcript_id in transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region:
        gene=gene_info[transcript_id]["gene"]
        for selected_sample_filename_num,selected_sample_filename in enumerate(options.selected_sample):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if selected_sample_filename_num%2!=0:
                    pass
                else:
                    continue
            background_sample_filename=options.background_sample[selected_sample_filename_num]
            #print(transcript_id,gene,gene in enrichment_info)
            if all_info[selected_sample_filename][transcript_id]["num_reads_in_frame"]<2 or all_info[background_sample_filename][transcript_id]["num_reads_in_frame"]<2:continue
            calculate_in_frame_score(transcript_id,
                                     selected_sample_filename,
                                     all_info[selected_sample_filename][transcript_id]["num_reads_in_frame"],
                                     all_info[selected_sample_filename][transcript_id]["num_reads_out_of_frame"],
                                     all_info[background_sample_filename][transcript_id]["num_reads_in_frame"],
                                     all_info[background_sample_filename][transcript_id]["num_reads_out_of_frame"])
    """
    fhw=open(options.combined_graph_final,"w")
    fhw.write(",".join(headers)+"\n")
    for transcript_id in transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region:
        gene=gene_info[transcript_id]["gene"]
        for selected_sample_filename_num,selected_sample_filename in enumerate(options.selected_sample):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if selected_sample_filename_num%2!=0:
                    pass
                else:
                    continue
            background_sample_filename=options.background_sample[selected_sample_filename_num]
            #print(transcript_id,gene,gene in enrichment_info)
            info_about_transcript=[transcript_id,
                  selected_sample_filename,
                  gene_info[transcript_id]["cds_start"],
                  gene_info[transcript_id]["cds_end"],
                  str(len(transcriptome[transcript_id])),
                  all_info[selected_sample_filename][transcript_id]["num_reads_in_frame"],
                  all_info[selected_sample_filename][transcript_id]["num_reads_in_frame"]+all_info[selected_sample_filename][transcript_id]["num_reads_out_of_frame"],
                  all_info[background_sample_filename][transcript_id]["num_reads_in_frame"],
                  all_info[background_sample_filename][transcript_id]["num_reads_in_frame"]+all_info[background_sample_filename][transcript_id]["num_reads_out_of_frame"],
                  ";".join(all_info[selected_sample_filename][transcript_id]["reads_in_frame"]),
                  enrichment_info[gene]["log2FoldChange"],
                  enrichment_info[gene]["padj"],
                  enrichment_info[gene]["norm_counts"][options.selected_sample[selected_sample_filename_num-1]],
                  enrichment_info[gene]["norm_counts"][options.background_sample[selected_sample_filename_num-1]],
                  ";".join(list(map(str,transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region[transcript_id])))
                  ]
            fhw.write(",".join(list(map(str,info_about_transcript)))+"\n")
    fhw.close()

def calculate_in_frame_score(transcript_id,selected_sample_filename,num_reads_in_frame_s,num_reads_out_of_frame_s,num_reads_in_frame_ns,num_reads_out_of_frame_ns):
    """
    """
    
    """
    calculate_score <- function(count_table, alpha=0.05){
    data <- count_table
    data$in_frame_prop_s <- data$num_junction_reads_in_frame_s/data$num_junction_reads_s
    data$in_frame_prop_ns <- data$num_junction_reads_in_frame_ns/data$num_junction_reads_ns
    
    #with normal
    #If we have data from NS libraries
    data$in_frame_prop_ns_ho<- data$in_frame_prop_ns
    data$in_frame_prop_ns_ho[data$num_junction_reads_ns==0]<- 1/3
    data$statistic<- (data$in_frame_prop_s-data$in_frame_prop_ns_ho)/
      sqrt((data$in_frame_prop_s*(1-data$in_frame_prop_s))/(data$num_junction_reads_s+1)+
             (data$in_frame_prop_ns_ho*(1-data$in_frame_prop_ns_ho))/(data$num_junction_reads_ns+1))
    
    data$p_val <- pnorm(q=data$statistic,lower.tail = FALSE)
    data$rank <- rank(data$statistic)
    data$freq_score <- data$rank/max(data$rank)
    #data$freq_score <- pnorm(q=data$statistic) #1-data$p_val
    
    data[is.na(data$freq_score),"freq_score"] <- 0
    return(data)
    """
    if num_reads_in_frame_s<0:num_reads_in_frame_s=0
    if num_reads_out_of_frame_s<0:num_reads_out_of_frame_s=0
    if num_reads_in_frame_ns<0:num_reads_in_frame_ns=0
    if num_reads_out_of_frame_ns<0:num_reads_out_of_frame_ns=0
    
    num_junction_reads_s=num_reads_in_frame_s+num_reads_out_of_frame_s
    num_junction_reads_ns=num_reads_in_frame_ns+num_reads_out_of_frame_ns
    in_frame_proportion_s=num_reads_in_frame_s/num_junction_reads_s
    in_frame_proportion_ns=num_reads_in_frame_ns/num_junction_reads_ns
    
    if in_frame_proportion_ns==0:
        in_frame_proportion_ns=1/3
        
    test_statistic_numerator=in_frame_proportion_s-in_frame_proportion_ns
    test_statistic_denominator_part1=in_frame_proportion_s*(1-in_frame_proportion_s)/(num_junction_reads_s+1)
    test_statistic_denominator_part2=in_frame_proportion_ns*(1-in_frame_proportion_ns)/(num_junction_reads_ns+1)
    print(num_reads_in_frame_s,num_reads_out_of_frame_s,num_reads_in_frame_ns,num_reads_out_of_frame_ns)
    print(transcript_id,selected_sample_filename,test_statistic_numerator,test_statistic_denominator_part1,test_statistic_denominator_part2)
    sys.stdout.flush()
    test_statistic=test_statistic_numerator/math.sqrt(test_statistic_denominator_part1 + test_statistic_denominator_part2)
    
    #print(transcript_id,selected_sample_filename,stats.norm.ppf(test_statistic))
    sys.stdout.flush()

def generateRunReportFile(options):
    """
    Generates a complete report of the execution of the program
    This will record the duration of execution of each step and 
    also important details about each of the steps
    """
    fhw=open(options.run_details_info_csv,"w")
    all_information={}
    
    all_information[options.output_directory.split("/")[-1]]={}
    all_information[options.output_directory.split("/")[-1]]["genome_browser_viewing_bamfile"]=options.selected_sample_genome_browser
    all_information[options.output_directory.split("/")[-1]]["final_info_file"]=options.combined_graph_final
    all_information[options.output_directory.split("/")[-1]]["salmon_gene_raw_counts"]=options.salmon_gene_counts_matrix
    all_information[options.output_directory.split("/")[-1]]["deseq2_gene_norm_counts"]=options.deseq2_normalized_counts
    all_information[options.output_directory.split("/")[-1]]["run_details_info_csv"]=options.run_details_info_csv
    all_information[options.output_directory.split("/")[-1]]["record_time"]=options.record_time
    fhw.write(",")
    for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]):
        for file_num,filename in enumerate(eachtype):
            if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue
            fhw.write(filename+",")
            if filename not in all_information:
                all_information[filename]={}
            
            """
            Read trimming information
            """
            if num_eachtype==0:
                inputfilename=options.selected_sample_adapter_trimmed_error_file[file_num]
            else:
                inputfilename=options.background_sample_adapter_trimmed_error_file[file_num]
            fhr=open(inputfilename,"r")
            for line in fhr:
                if "Input Reads" in line:
                    line=line.strip()
                    total_reads=int(line.split("Input Reads:")[-1].split()[0].strip())
                    reads_dropped=int(line.split("Dropped:")[-1].split()[0].strip())
                    reads_surviving=int(line.split("Surviving:")[-1].split()[0].strip())
                    all_information[filename]["total_reads"]=total_reads
                    all_information[filename]["num_reads_dropped"]=reads_dropped
                    all_information[filename]["num_reads_after_adapter_trimming"]=reads_surviving
            fhr.close()
            
            """
            Number of reads mapped for trimming
            """
            if num_eachtype==0:
                logfilename=options.selected_sample_STAR_prefix_round1[file_num]+"Log.final.out"
            else:
                logfilename=options.background_sample_STAR_prefix_round1[file_num]+"Log.final.out"
            fhr=open(logfilename,"r")
            for line in fhr:
                if "Uniquely mapped reads" in line:
                    umr=line.strip().split("|")[-1].strip()[:-1]
                elif "% of reads mapped to multiple loci" in line:
                    mmr=line.strip().split("|")[-1].strip()[:-1]
                elif "% of reads mapped to too many loci" in line:
                    mmur=line.strip().split("|")[-1].strip()[:-1]
                elif "% of reads unmapped: too many mismatches" in line or "% of reads unmapped: too short" in line or "% of reads unmapped: other" in line:
                    ur=line.strip().split("|")[-1].strip()[:-1]
            fhr.close()
            all_information[filename]["umr"]=umr
            all_information[filename]["mmr"]=mmr
            all_information[filename]["mmur"]=mmur
            all_information[filename]["ur"]=ur
            
            """
            Number of reads having vector sequence
            """
            if num_eachtype==0:
                inputfilename=options.selected_sample_trimming_stats[file_num]
            else:
                inputfilename=options.background_sample_trimming_stats[file_num]
            #print(inputfilename)
            fhr=open(inputfilename,"r")
            five_prime_forward,five_prime_reverse,five_prime_vector_whole_read,three_prime_forward,three_prime_reverse,three_prime_vector_whole_read,yeast_plasmid,yeast_chromosome,Reads_removed_due_to_small_size,Reads_removed_due_to_polyA,num_junction_reads=[int(line.split()[-1]) for line in fhr if " " in line]
            all_information[filename]["five_prime_forward_vector"]=five_prime_forward
            all_information[filename]["five_prime_reverse_vector"]=five_prime_reverse
            all_information[filename]["three_prime_forward_vector"]=three_prime_forward
            all_information[filename]["three_prime_reverse_vector"]=three_prime_reverse
            all_information[filename]["hits_to_yeast_chromosome"]=yeast_chromosome
            all_information[filename]["hits_to_yeast_plasmid"]=yeast_plasmid
            all_information[filename]["five_prime_vector_whole_read"]=five_prime_vector_whole_read
            all_information[filename]["three_prime_vector_whole_read"]=three_prime_vector_whole_read
            all_information[filename]["Reads_removed_due_to_small_size"]=Reads_removed_due_to_small_size
            all_information[filename]["Reads_removed_due_to_polyA"]=Reads_removed_due_to_polyA
            all_information[filename]["num_junction_reads"]=num_junction_reads
            #all_information[filename]["num_all_reads"]=num_all_reads
            fhr.close()
            
            """
            Genome Browser viewing files
            """
            if num_eachtype==0:
                all_information[filename]["genome_browser_viewing_bamfile_per_replicate"]=options.selected_sample_genome_browser_per_replicate[file_num]
            else:
                all_information[filename]["genome_browser_viewing_bamfile_per_replicate"]=""
    #pprint.pprint(all_information)
    """if options.selected_ended=="PE" and options.background_ended=="PE":
                if file_num%2==1:
                    pass
                else:
                    continue"""
    if options.selected_ended=="PE" and options.background_ended=="PE":
        fhw.write("\n")
        fhw.write("Total reads,"+",".join(list(map(str,[all_information[filename]["total_reads"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1 ]))))
        fhw.write("\n")
        fhw.write("Reads retained after Adapter Trimming,"+",".join(list(map(str,[all_information[filename]["num_reads_after_adapter_trimming"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1 ]))))
        fhw.write("\n")
        fhw.write("% of Uniquely mapped reads,"+",".join(list(map(str,[all_information[filename]["umr"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1 ]))))
        fhw.write("\n")
        fhw.write("% of Multi mapped reads,"+",".join(list(map(str,[all_information[filename]["mmr"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("% of reads unmapped due to too many loci,"+",".join(list(map(str,[all_information[filename]["mmur"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("% of reads unmapped,"+",".join(list(map(str,[all_information[filename]["ur"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 5' vector in forward orientation,"+",".join(list(map(str,[all_information[filename]["five_prime_forward_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 5' vector in reverse orientation,"+",".join(list(map(str,[all_information[filename]["five_prime_reverse_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 3' vector in forward orientation,"+",".join(list(map(str,[all_information[filename]["three_prime_forward_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 3' vector in reverse orientation,"+",".join(list(map(str,[all_information[filename]["three_prime_reverse_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of reads mapped to yeast plasmid,"+",".join(list(map(str,[all_information[filename]["hits_to_yeast_plasmid"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due to entire 5' vector,"+",".join(list(map(str,[all_information[filename]["five_prime_vector_whole_read"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due to entire 3' vector,"+",".join(list(map(str,[all_information[filename]["three_prime_vector_whole_read"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due very small size,"+",".join(list(map(str,[all_information[filename]["Reads_removed_due_to_small_size"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due to polyA,"+",".join(list(map(str,[all_information[filename]["Reads_removed_due_to_polyA"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads,"+",".join(list(map(str,[all_information[filename]["num_junction_reads"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))))
        fhw.write("\n")
        fhw.write("Location of bamfile for viewing on Genome Browser,"+",".join([all_information[filename]["genome_browser_viewing_bamfile_per_replicate"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) if file_num%2==1]))
        fhw.write("\n")
        fhw.write("Location of bamfile for all replicates,"+all_information[options.output_directory.split("/")[-1]]["genome_browser_viewing_bamfile"])
        fhw.write("\n")
        fhw.write("Location of reports file,"+all_information[options.output_directory.split("/")[-1]]["final_info_file"])
        fhw.write("\n")
        fhw.write("Salmon raw counts,"+all_information[options.output_directory.split("/")[-1]]["salmon_gene_raw_counts"])
        fhw.write("\n")
        fhw.write("DESeq2 normalized countsfile,"+all_information[options.output_directory.split("/")[-1]]["deseq2_gene_norm_counts"])
        fhw.write("\n")
    else:
        fhw.write("\n")
        fhw.write("Total reads,"+",".join(list(map(str,[all_information[filename]["total_reads"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Reads retained after Adapter Trimming,"+",".join(list(map(str,[all_information[filename]["num_reads_after_adapter_trimming"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("% of Uniquely mapped reads,"+",".join(list(map(str,[all_information[filename]["umr"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("% of Multi mapped reads,"+",".join(list(map(str,[all_information[filename]["mmr"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("% of reads unmapped due to too many loci,"+",".join(list(map(str,[all_information[filename]["mmur"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("% of reads unmapped,"+",".join(list(map(str,[all_information[filename]["ur"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 5' vector in forward orientation,"+",".join(list(map(str,[all_information[filename]["five_prime_forward_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 5' vector in reverse orientation,"+",".join(list(map(str,[all_information[filename]["five_prime_reverse_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 3' vector in forward orientation,"+",".join(list(map(str,[all_information[filename]["three_prime_forward_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads with 3' vector in reverse orientation,"+",".join(list(map(str,[all_information[filename]["three_prime_reverse_vector"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of reads mapped to yeast plasmid,"+",".join(list(map(str,[all_information[filename]["hits_to_yeast_plasmid"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due to entire 5' vector,"+",".join(list(map(str,[all_information[filename]["five_prime_vector_whole_read"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due to entire 3' vector,"+",".join(list(map(str,[all_information[filename]["three_prime_vector_whole_read"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due very small size,"+",".join(list(map(str,[all_information[filename]["Reads_removed_due_to_small_size"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of reads discarded due to polyA,"+",".join(list(map(str,[all_information[filename]["Reads_removed_due_to_polyA"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Num of fusion reads,"+",".join(list(map(str,[all_information[filename]["num_junction_reads"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))))
        fhw.write("\n")
        fhw.write("Location of bamfile for viewing on Genome Browser,"+",".join([all_information[filename]["genome_browser_viewing_bamfile_per_replicate"] for num_eachtype,eachtype in enumerate([options.selected_sample,options.background_sample]) for file_num,filename in enumerate(eachtype) ]))
        
        fhw.write("Location of bamfile for all replicates,"+all_information[options.output_directory.split("/")[-1]]["genome_browser_viewing_bamfile"])
        fhw.write("\n")
        fhw.write("Location of reports file,"+all_information[options.output_directory.split("/")[-1]]["final_info_file"])
        fhw.write("\n")
        fhw.write("Salmon raw counts,"+all_information[options.output_directory.split("/")[-1]]["salmon_gene_raw_counts"])
        fhw.write("\n")
        fhw.write("DESeq2 normalized countsfile,"+all_information[options.output_directory.split("/")[-1]]["deseq2_gene_norm_counts"])
        fhw.write("\n")
    
    fhw.write("Running times")
    fhw.write("\n")
    for eachfunction in all_information[options.output_directory.split("/")[-1]]["record_time"]:
        fhw.write(eachfunction+","+all_information[options.output_directory.split("/")[-1]]["record_time"][eachfunction].replace(",",":"))
        fhw.write("\n")
    fhw.close()

def createTranscriptomeFileForPrimerDesign(options,gene_info):
    """
    """
    transcriptome=readFastaFile(options.transcriptome)
    if os.path.exists(options.functional_annotation)==True:
        functional_annotation={}
        fhr=open(options.functional_annotation,"r")
        for line in fhr:
            functional_annotation[line.strip().split()[0]]=line.strip().split()[1] if len(line.strip().split())>1 else ""
        fhr.close()
        
    #print(options.combined_graph_final)
    sys.stdout.flush()
    fhr=open(options.combined_graph_final,"r")
    fhw=open(options.design_primers_for_transcript,"w")
    for line_num,line in enumerate(fhr):
        if line_num==0:continue
        try:
            Transcript_id,Replicate,CDS_start,CDS_end,Length,num_fusion_reads_in_frame_selected_sample,num_fusion_reads_selected_sample,num_fusion_reads_in_frame_background_sample,num_fusion_reads_background_sample,starting_location_of_in_frame_junction_reads,log2FoldChange,padj,norm_counts_selected_sample,norm_counts_background_sample,location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS=line.strip().split(",")
        except ValueError:
            print(line)
            return
        header=Transcript_id+"_"+Replicate+"_"+log2FoldChange+"_"+padj+"_"+CDS_start+"_"+CDS_end
        gene=gene_info[Transcript_id]["gene"]
        CDS_start=int(CDS_start)
        CDS_end=int(CDS_end)
        seq=transcriptome[Transcript_id]
        in_frame_fusion_seq=[" " for i in range(len(seq))]
        stop_codons=[" " for i in range(len(seq))]
        if location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS!="":
            location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS=list(map(int,location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS.split(";")))
        else:
            location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS=""
        if len(starting_location_of_in_frame_junction_reads)>0:
            starting_location_of_in_frame_junction_reads=list(map(int,list(set([ele.split(":")[1] for ele in starting_location_of_in_frame_junction_reads.split(";")]))))
        else:
            starting_location_of_in_frame_junction_reads=-1
        if starting_location_of_in_frame_junction_reads!=-1:
            for eachloc in starting_location_of_in_frame_junction_reads:
                if eachloc >= CDS_end:
                    print(line)
                    print("TROUBLE")
                """print(location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS)
                sys.stdout.flush()"""
                in_frame_fusion_seq[eachloc-1]="*" if 0 not in list(set([(ele-eachloc)%3  for ele in location_of_STOP_codons_in_5_prime_UTR_region_in_frame_with_CDS if eachloc<ele])) else "X"
        else:
            continue
        in_frame_fusion_seq="".join(in_frame_fusion_seq)
        fhw.write(header+"\n")
        fhw.write(seq[:CDS_start-1].lower()+seq[CDS_start-1:CDS_end]+seq[CDS_end:].lower()+"\n")
        fhw.write(in_frame_fusion_seq[:CDS_start-1].lower()+in_frame_fusion_seq[CDS_start-1:CDS_end]+in_frame_fusion_seq[CDS_end:].lower()+"\n")
    fhr.close()
    fhw.close()
    
def configureLogger(options):
    """
    """
    os.system("mkdir -p "+options.output_directory)
    os.system("rm "+options.output_directory+"/progress.log")
    
    arguments={}
    arguments["file_name"]=options.output_directory+"/progress.log"
    arguments["formatter"] = "%(asctime)s - %(name)s - %(levelname)6s - %(message)s"
    arguments["level"]     = logging.DEBUG
    arguments["delay"]     = False
    
    (logger_proxy,logging_mutex) = make_shared_logger_and_proxy (setup_std_shared_logger,"PRIDE", arguments)
    
    return logger_proxy,logging_mutex

def main():
    commandLineArg=sys.argv
    if len(commandLineArg)==1:
        print("Please use the --help option to get usage information")
    
    current_time=time.time()
    options=parseCommandLineArguments()
    options=populateOptionsDataStructure(options,options.all_arguments)
    logger_proxy,logging_mutex=configureLogger(options)
    
    options=analyzeCommandLineArguments(options,logger_proxy,logging_mutex)
    options.record_time["analyzeCommandLineArguments"]=str(convertToDateAndTime(int(time.time()-current_time)))
    
    current_time=time.time()
    generateTranscriptToGeneMap(options.gtf,options)
    options.record_time["generateTranscriptToGeneMap"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Transcripts mapped to Genes")
    #logger.info('Function generateTranscriptToGeneMap completed')
    
    current_time=time.time()
    gene_info=getGeneInfo(options.transcriptome,options.transcript_to_gene_map)
    options.record_time["getGeneInfo"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Gene info extracted")
    #logger.info("Function getGeneInfo completed")
    
    current_time=time.time()
    transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region=findTranscriptsWithSTOPCodonsIn5PrimeUTRSequence(options,gene_info)
    options.record_time["findTranscriptsWithSTOPCodonsIn5PrimeUTRSequence"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Transcripts with STOP codons in 5 prime UTR detected")
    #logger.info("Function findTranscriptsWithSTOPCodonsIn5PrimeUTRSequence completed")
    
    current_time=time.time()
    runTrimmomaticToTrimOffAdapters(options,logger_proxy,logging_mutex)
    options.record_time["runTrimmomaticToTrimOffAdapters"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Trimmomatic run to trim off adapters completed")
    #logger.info("Function runTrimmomaticToTrimOffAdapters completed")
    
    current_time=time.time()
    removeNFromFastq(options,logger_proxy,logging_mutex)
    options.record_time["removeNFromFastq"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Removal of leading and trailing Ns from fastq completed")
    #logger.info("Function removeNFromFastq completed")
    
    current_time=time.time()
    alignReadsWithStarForTrimming(options,logger_proxy,logging_mutex)
    options.record_time["alignReadsWithStarForTrimming"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("STAR mapping completed")
    #logger.info("Function alignReadsWithStarForTrimming completed")
    
    current_time=time.time()
    findReadsWithVectorSequenceAndTrim(options,logger_proxy,logging_mutex)
    options.record_time["findReadsWithVectorSequenceAndTrim"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Fusion read detection for all sequences are completed")
    #logger.info("Function findReadsWithVectorSequenceAndTrim completed")
    
    current_time=time.time()
    reAlignReadsMappedToVector(options)
    options.record_time["reAlignReadsMappedToVector"]=str(convertToDateAndTime(int(time.time()-current_time)))
    #logger.info("Function reAlignReadsMappedToVector completed")
    
    current_time=time.time()
    prepareGenomeFilesForGenomeBrowser(options,logger_proxy,logging_mutex)
    options.record_time["prepareGenomeFilesForGenomeBrowser"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Generation of files for viewing on genome browser completed")
    #logger.info("Function prepareGenomeFilesForGenomeBrowser completed")
    
    current_time=time.time()
    prepareTranscriptomeFiles(options,logger_proxy,logging_mutex)
    options.record_time["prepareTranscriptomeFiles"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Generation of transcriptomic files completed")
    #logger.info("Function prepareTranscriptomeFiles completed")
    
    current_time=time.time()
    runSalmonToGenerateCounts(options,logger_proxy,logging_mutex)
    options.record_time["runSalmonToGenerateCounts"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Generation of gene counts with Salmon completed")
    #logger.info("Function runSalmonToGenerateCounts completed")
    
    current_time=time.time()
    performEnrichmentAnalysis(options)
    options.record_time["performEnrichmentAnalysis"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Enrichment analysis with DESeq2 completed")
    #logger.info("Function performEnrichmentAnalysis completed")
    
    current_time=time.time()
    generateReportFile(options,gene_info,transcripts_to_in_frame_STOP_codon_locations_in_5_prime_UTR_region)
    options.record_time["generateReportFile"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Generation of report file for all transcripts completed")
    #logger.info("Function generateReportFile completed")
    
    current_time=time.time()
    createTranscriptomeFileForPrimerDesign(options,gene_info)
    options.record_time["createTranscriptomeFileForPrimerDesign"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Generation of file for primer design completed")
    #logger.info("Function createTranscriptomeFileForPrimerDesign completed")
    
    current_time=time.time()
    generateRunReportFile(options)
    options.record_time["generateRunReportFile"]=str(convertToDateAndTime(int(time.time()-current_time)))
    with logging_mutex:
        logger_proxy.info("Generation of report file for run completed")
    #logger.info("Function generateRunReportFile completed")
    
    with logging_mutex:
        logger_proxy.info("All functions were executed successfully")
    #logger.info("All runs completed successfully")
    

if __name__ == "__main__":
    main()
    
    